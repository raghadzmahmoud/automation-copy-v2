#!/usr/bin/env python3
"""
üì• News Scraper Job (Condition-Based)

‚ö†Ô∏è ŸÑÿß ŸäŸàÿ¨ÿØ ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿßŸÑŸàŸÇÿ™ ŸáŸÜÿß!
   ÿßŸÑŸÄ Scheduler (worker.py) ŸáŸà ÿßŸÑŸÖÿ™ÿ≠ŸÉŸÖ ÿ®ÿßŸÑŸàŸÇÿ™

Behavior:
- Ÿäÿ≥ÿ≠ÿ® ŸÖŸÜ ŸÉŸÑ ÿßŸÑŸÖÿµÿßÿØÿ± ÿßŸÑŸÜÿ¥ÿ∑ÿ©
- Ÿäÿ™ÿ≠ŸÇŸÇ ŸÅŸÇÿ∑ ŸÖŸÜ: ŸáŸÑ ÿßŸÑŸÖÿµÿØÿ± ÿ¨ÿßŸáÿ≤ ŸÑŸÑÿ≥ÿ≠ÿ®ÿü (minutes_since_fetch)
- 8 ÿ£ÿÆÿ®ÿßÿ± ŸÖŸÜ ŸÉŸÑ ŸÖÿµÿØÿ±
- ‚úÖ ÿ®ÿπÿØ ÿßŸÑÿ≠ŸÅÿ∏: ŸäÿπŸÖŸÑ enqueue ŸÑŸÑŸÄ clustering ŸÅŸä news_pipeline_queue

Usage: Called by worker.py (cron-based scheduled_tasks)
"""

import sys
import os

sys.path.insert(
    0,
    os.path.dirname(
        os.path.dirname(
            os.path.dirname(os.path.abspath(__file__))
        )
    )
)

import logging
from datetime import datetime
import psycopg2
from settings import DB_CONFIG
from app.config.user_config import user_config

# =============================================================================
# LOGGING
# =============================================================================

log_dir = os.path.join(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
    'logs'
)
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'scraper_job.log'), encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# =============================================================================
# DATABASE
# =============================================================================

def is_processing_pipeline_running() -> bool:
    """
    ÿ™ÿ≠ŸÇŸÇ ÿ•ÿ∞ÿß ŸÅŸä processing pipeline ÿ¥ÿ∫ÿßŸÑ ÿ≠ÿßŸÑŸäŸãÿß
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # ÿ™ÿ≠ŸÇŸÇ ÿ•ÿ∞ÿß ŸÅŸä processing_pipeline ÿ¥ÿ∫ÿßŸÑ ŸÖŸÜ ÿ¢ÿÆÿ± 30 ÿØŸÇŸäŸÇÿ©
        cursor.execute("""
            SELECT COUNT(*) FROM scheduled_task_logs stl
            JOIN scheduled_tasks st ON stl.scheduled_task_id = st.id
            WHERE st.task_type = 'processing_pipeline'
            AND stl.executed_at >= NOW() - INTERVAL '30 minutes'
            AND stl.status = 'running'
        """)
        
        running_count = cursor.fetchone()[0]
        cursor.close()
        conn.close()
        
        return running_count > 0
        
    except Exception as e:
        logger.error(f"Error checking pipeline status: {e}")
        return False


def enqueue_news_for_clustering(news_ids: list) -> int:
    """
    ‚úÖ ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑÿ¨ÿØŸäÿØÿ© ŸÑŸÄ news_pipeline_queue ‚Üí clustering

    ŸäŸèÿ≥ÿ™ÿØÿπŸâ ÿ®ÿπÿØ ÿ≠ŸÅÿ∏ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸÑÿ•ÿ∑ŸÑÿßŸÇ ÿßŸÑŸÄ real-time pipeline.
    Ÿäÿ≥ÿ™ÿÆÿØŸÖ ON CONFLICT DO NOTHING ŸÑÿ™ÿ¨ŸÜÿ® ÿßŸÑÿ™ŸÉÿ±ÿßÿ±.

    Args:
        news_ids: ŸÇÿßÿ¶ŸÖÿ© IDs ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ© ÿ≠ÿØŸäÿ´ÿßŸã

    Returns:
        ÿπÿØÿØ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖŸèÿ∂ÿßŸÅÿ© ŸÅÿπŸÑÿßŸã ŸÑŸÑŸÄ queue
    """
    if not news_ids:
        return 0

    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        count = 0

        for news_id in news_ids:
            cursor.execute("""
                INSERT INTO news_pipeline_queue (news_id, stage, status, next_run_at)
                VALUES (%s, 'clustering', 'pending', NOW())
                ON CONFLICT DO NOTHING
            """, (news_id,))
            count += cursor.rowcount

        conn.commit()
        cursor.close()
        conn.close()

        if count > 0:
            logger.info(f"üì¨ Enqueued {count} news items for clustering")

        return count

    except psycopg2.errors.UndefinedTable:
        # ÿ¨ÿØŸàŸÑ news_pipeline_queue ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ ÿ®ÿπÿØ (ŸÇÿ®ŸÑ ÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑŸÄ migration)
        logger.warning("‚ö†Ô∏è  news_pipeline_queue table not found - run migration first")
        return 0
    except Exception as e:
        logger.error(f"‚ùå Error enqueuing news for clustering: {e}")
        return 0


def get_active_sources():
    """
    Get all active sources ready for scraping
    Condition: minutes_since_fetch >= DEFAULT_INTERVAL
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT
                s.id,
                s.name,
                s.source_type_id,
                s.url,
                COALESCE(s.last_fetched, '1970-01-01'::timestamp),
                EXTRACT(EPOCH FROM (
                    CURRENT_TIMESTAMP - COALESCE(s.last_fetched, '1970-01-01')
                )) / 60 AS minutes_since_fetch,
                st.name AS source_type_name
            FROM sources s
            LEFT JOIN source_types st ON s.source_type_id = st.id
            WHERE s.is_active = true
            ORDER BY COALESCE(s.last_fetched, '1970-01-01') ASC
        """)

        sources = cursor.fetchall()
        cursor.close()
        conn.close()

        # Filter by interval (condition-based)
        DEFAULT_INTERVAL = getattr(
            user_config,
            'default_fetch_interval_minutes',
            30  # default: 30 minutes between fetches per source
        )

        return [s for s in sources if s[5] >= DEFAULT_INTERVAL]

    except Exception as e:
        logger.error(f"Error getting active sources: {e}")
        return []


# =============================================================================
# MAIN
# =============================================================================

def scrape_news() -> dict:
    """
    Main scraping function
    
    Returns:
        dict with stats: {total, success, failed, news_saved}
    """
    start_time = datetime.now()
    
    logger.info("=" * 60)
    logger.info(f"üì• Scraper Job started at {start_time}")
    logger.info("=" * 60)

    # Check if enabled
    if not user_config.scraping_enabled:
        logger.info("‚è≠Ô∏è Scraping is disabled in configuration")
        return {'total': 0, 'success': 0, 'failed': 0, 'news_saved': 0, 'skipped': True}

    # ‚úÖ Check if processing pipeline is running
    if is_processing_pipeline_running():
        logger.info("‚è≠Ô∏è Processing pipeline is running, skipping scraping to avoid conflicts")
        logger.info("=" * 60)
        return {'total': 0, 'success': 0, 'failed': 0, 'news_saved': 0, 'skipped': True}

    # Get sources ready for scraping
    sources = get_active_sources()
    
    if not sources:
        logger.info("‚è≠Ô∏è No sources ready for scraping (all recently fetched)")
        return {'total': 0, 'success': 0, 'failed': 0, 'news_saved': 0, 'skipped': True}
    
    logger.info(f"üìã Found {len(sources)} sources ready for scraping:")
    for i, s in enumerate(sources, start=1):
        logger.info(f"   {i}. [{s[6]}] {s[1]} - {s[3][:50]}...")

    # Import scraper
    from app.services.ingestion.scraper import scrape_url

    total_news = 0
    success_count = 0
    failed_count = 0
    all_saved_ids = []   # ‚úÖ ÿ¨ŸÖÿπ IDs ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ© ŸÑŸÑŸÄ enqueue

    for source in sources:
        source_id = source[0]
        name = source[1]
        url = source[3]
        source_type = source[6]

        try:
            logger.info(f"üì• Scraping: {name} ({source_type})")

            result = scrape_url(
                url=url,
                save_to_db=True,
                max_articles=8,
                language_id=1,
                use_telegram_api=False
            )

            if result.success:
                total_news += result.saved
                success_count += 1
                logger.info(
                    f"   ‚úÖ Extracted={result.extracted}, "
                    f"Saved={result.saved}, Skipped={result.skipped}"
                )
                # ‚úÖ ÿ¨ŸÖÿπ IDs ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ© (ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸÖÿ™ÿßÿ≠ÿ©)
                if hasattr(result, 'saved_ids') and result.saved_ids:
                    all_saved_ids.extend(result.saved_ids)
            else:
                failed_count += 1
                logger.warning(f"   ‚ö†Ô∏è {result.error}")

        except Exception as e:
            failed_count += 1
            logger.error(f"   ‚ùå Error: {e}")

    # ‚úÖ Enqueue ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑÿ¨ÿØŸäÿØÿ© ŸÑŸÑŸÄ real-time pipeline
    enqueued_count = 0
    if all_saved_ids:
        # ‚úÖ ÿßŸÑÿ≠ÿßŸÑÿ© ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ©: IDs ŸÖÿ™ÿßÿ≠ÿ© ŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸÖŸÜ ÿßŸÑŸÄ scraper
        logger.info(f"üì¨ Enqueuing {len(all_saved_ids)} news items for clustering pipeline...")
        enqueued_count = enqueue_news_for_clustering(all_saved_ids)
    elif total_news > 0:
        # ‚ö†Ô∏è Safety net: ŸÜÿßÿØÿ±ÿßŸã ŸÖÿß Ÿäÿ≠ÿØÿ´ ÿ®ÿπÿØ ÿ•ÿ∂ÿßŸÅÿ© saved_ids ŸÑŸÑŸÄ ScrapeResult
        logger.warning("‚ö†Ô∏è saved_ids not available, using DB fallback")
        enqueued_count = _enqueue_latest_news(total_news)

    # Summary
    duration = (datetime.now() - start_time).total_seconds()
    
    logger.info("=" * 60)
    logger.info(f"üìä Scraping completed in {duration:.2f}s")
    logger.info(f"   ‚úÖ Successful: {success_count}")
    logger.info(f"   ‚ùå Failed: {failed_count}")
    logger.info(f"   üì∞ News saved: {total_news}")
    if enqueued_count > 0:
        logger.info(f"   üì¨ Enqueued for pipeline: {enqueued_count}")
    logger.info("=" * 60)

    return {
        'total': len(sources),
        'success': success_count,
        'failed': failed_count,
        'news_saved': total_news,
        'enqueued': enqueued_count,
        'duration': duration,
        'skipped': False
    }


def _enqueue_latest_news(limit: int) -> int:
    """
    ‚ö†Ô∏è Safety net fallback: ÿ¨ŸÑÿ® ÿ¢ÿÆÿ± ÿßŸÑÿ£ÿÆÿ®ÿßÿ± Ÿàÿ•ÿ∂ÿßŸÅÿ™Ÿáÿß ŸÑŸÑŸÄ queue.

    ÿ®ÿπÿØ ÿ•ÿ∂ÿßŸÅÿ© saved_ids ŸÑŸÄ ScrapeResultÿå Ÿáÿ∞ÿß ÿßŸÑŸÄ fallback
    ŸÑŸÜ ŸäŸèÿ≥ÿ™ÿØÿπŸâ ÿ•ŸÑÿß ŸÅŸä ÿ≠ÿßŸÑÿßÿ™ ÿßÿ≥ÿ™ÿ´ŸÜÿßÿ¶Ÿäÿ© ÿ¨ÿØÿßŸã.

    Args:
        limit: ÿπÿØÿØ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ŸÑŸÑÿ¨ŸÑÿ®

    Returns:
        ÿπÿØÿØ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖŸèÿ∂ÿßŸÅÿ© ŸÑŸÑŸÄ queue
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        # ÿ¨ŸÑÿ® ÿ¢ÿÆÿ± ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ© ÿßŸÑŸÑŸä ŸÖÿß ÿØÿÆŸÑÿ™ ÿßŸÑŸÄ queue ÿ®ÿπÿØ
        cursor.execute("""
            SELECT rn.id
            FROM raw_news rn
            WHERE rn.collected_at >= NOW() - INTERVAL '10 minutes'
              AND NOT EXISTS (
                  SELECT 1 FROM news_pipeline_queue npq
                  WHERE npq.news_id = rn.id
                    AND npq.stage = 'clustering'
              )
            ORDER BY rn.collected_at DESC
            LIMIT %s
        """, (limit,))

        rows = cursor.fetchall()
        cursor.close()
        conn.close()

        if not rows:
            return 0

        news_ids = [r[0] for r in rows]
        return enqueue_news_for_clustering(news_ids)

    except psycopg2.errors.UndefinedTable:
        logger.warning("‚ö†Ô∏è  news_pipeline_queue table not found - run migration first")
        return 0
    except Exception as e:
        logger.error(f"‚ùå Error in _enqueue_latest_news: {e}")
        return 0


if __name__ == "__main__":
    scrape_news()