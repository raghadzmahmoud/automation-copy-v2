#!/usr/bin/env python3
"""
ðŸ”„ Processing Pipeline Job - Sequential Execution

ÙŠØ´ØªØºÙ„ ÙƒÙ„ 20 Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙŠØ¹Ù…Ù„ ÙƒÙ„ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨:
1. Clustering
2. Report Generation  
3. Social Media
4. Image Generation
5. Audio Generation

ÙƒÙ„ Ù…Ø±Ø­Ù„Ø© ØªØ´ØªØºÙ„ Ø¨Ø³ Ø¥Ø°Ø§ Ø§Ù„Ù„ÙŠ Ù‚Ø¨Ù„Ù‡Ø§ Ø®Ù„Øµ
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
from datetime import datetime
import psycopg2
from settings import DB_CONFIG
from app.config.user_config import user_config

# Logging setup
log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'processing_pipeline.log'), encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


# =============================================================================
# PIPELINE STATE CHECK
# =============================================================================

def is_pipeline_running() -> bool:
    """
    ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙÙŠ pipeline Ø´ØºØ§Ù„ Ø­Ø§Ù„ÙŠÙ‹Ø§
    Ù†Ø³ØªØ®Ø¯Ù… Ø¬Ø¯ÙˆÙ„ scheduled_task_logs Ù„Ù„ØªØ­Ù‚Ù‚
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙÙŠ processing_pipeline Ø´ØºØ§Ù„ Ù…Ù† Ø¢Ø®Ø± 30 Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ„Ø³Ø§ Ù…Ø§ Ø®Ù„Øµ
        cursor.execute("""
            SELECT COUNT(*) FROM scheduled_task_logs stl
            JOIN scheduled_tasks st ON stl.scheduled_task_id = st.id
            WHERE st.task_type = 'processing_pipeline'
            AND stl.executed_at >= NOW() - INTERVAL '30 minutes'
            AND stl.status = 'running'
        """)
        
        running_count = cursor.fetchone()[0]
        cursor.close()
        conn.close()
        
        return running_count > 0
        
    except Exception as e:
        logger.error(f"Error checking pipeline status: {e}")
        return False


def mark_pipeline_start() -> int:
    """
    Ø³Ø¬Ù„ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù€ pipeline
    Returns: log_id Ù„Ù„ØªØ­Ø¯ÙŠØ« Ù„Ø§Ø­Ù‚Ù‹Ø§
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ task_id Ù„Ù„Ù€ processing_pipeline
        cursor.execute("""
            SELECT id FROM scheduled_tasks 
            WHERE task_type = 'processing_pipeline'
        """)
        
        task_row = cursor.fetchone()
        if not task_row:
            logger.error("processing_pipeline task not found in scheduled_tasks")
            return None
        
        task_id = task_row[0]
        
        # Ø³Ø¬Ù„ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ°
        cursor.execute("""
            INSERT INTO scheduled_task_logs 
            (scheduled_task_id, status, executed_at)
            VALUES (%s, 'running', NOW())
            RETURNING id
        """, (task_id,))
        
        log_id = cursor.fetchone()[0]
        conn.commit()
        cursor.close()
        conn.close()
        
        return log_id
        
    except Exception as e:
        logger.error(f"Error marking pipeline start: {e}")
        return None


def mark_pipeline_end(log_id: int, status: str, duration: float, error: str = None):
    """
    Ø³Ø¬Ù„ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù€ pipeline
    """
    if not log_id:
        return
        
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE scheduled_task_logs 
            SET status = %s, 
                execution_time_seconds = %s,
                error_message = %s
            WHERE id = %s
        """, (status, duration, error, log_id))
        
        conn.commit()
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"Error marking pipeline end: {e}")


# =============================================================================
# PIPELINE EXECUTION
# =============================================================================

def run_processing_pipeline() -> dict:
    """
    Main pipeline function - ÙŠØ´ØºÙ„ ÙƒÙ„ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
    """
    start_time = datetime.now()
    
    logger.info("=" * 80)
    logger.info(f"ðŸ”„ Processing Pipeline started at {start_time}")
    logger.info("=" * 80)
    
    # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙÙŠ pipeline Ø´ØºØ§Ù„
    if is_pipeline_running():
        logger.info("â­ï¸ Pipeline already running, skipping this cycle")
        logger.info("=" * 80)
        return {'skipped': True, 'reason': 'already_running'}
    
    # Ø³Ø¬Ù„ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù€ pipeline
    log_id = mark_pipeline_start()
    
    results = {
        'skipped': False,
        'stages': {},
        'total_duration': 0,
        'success': True
    }
    
    try:
        # Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
        stages = [
            ('clustering', 'Clustering'),
            ('reports', 'Report Generation'),
            ('social_media', 'Social Media'),
            ('images', 'Image Generation'),
            ('audio', 'Audio Generation')
        ]
        
        for stage_key, stage_name in stages:
            stage_start = datetime.now()
            logger.info(f"ðŸ”„ Starting: {stage_name}")
            
            try:
                stage_result = run_stage(stage_key)
                stage_duration = (datetime.now() - stage_start).total_seconds()
                
                results['stages'][stage_key] = {
                    'success': not stage_result.get('error'),
                    'duration': stage_duration,
                    'result': stage_result
                }
                
                if stage_result.get('error'):
                    logger.error(f"âŒ {stage_name} failed: {stage_result['error']}")
                    results['success'] = False
                else:
                    logger.info(f"âœ… {stage_name} completed in {stage_duration:.2f}s")
                
            except Exception as e:
                stage_duration = (datetime.now() - stage_start).total_seconds()
                logger.error(f"âŒ {stage_name} crashed: {e}")
                
                results['stages'][stage_key] = {
                    'success': False,
                    'duration': stage_duration,
                    'error': str(e)
                }
                results['success'] = False
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        total_duration = (datetime.now() - start_time).total_seconds()
        results['total_duration'] = total_duration
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_status = 'completed' if results['success'] else 'failed'
        mark_pipeline_end(log_id, final_status, total_duration)
        
        logger.info("=" * 80)
        logger.info(f"ðŸ Pipeline {final_status} in {total_duration:.2f}s")
        
        # Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        for stage_key, stage_data in results['stages'].items():
            status = "âœ…" if stage_data['success'] else "âŒ"
            logger.info(f"   {status} {stage_key}: {stage_data['duration']:.2f}s")
        
        logger.info("=" * 80)
        
        return results
        
    except Exception as e:
        total_duration = (datetime.now() - start_time).total_seconds()
        error_msg = str(e)
        
        mark_pipeline_end(log_id, 'failed', total_duration, error_msg)
        
        logger.error(f"âŒ Pipeline crashed: {e}")
        logger.info("=" * 80)
        
        return {
            'skipped': False,
            'success': False,
            'error': error_msg,
            'total_duration': total_duration
        }


def run_stage(stage: str) -> dict:
    """
    ØªØ´ØºÙŠÙ„ Ù…Ø±Ø­Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„Ù€ pipeline
    """
    try:
        if stage == 'clustering':
            from app.jobs.clustering_job import cluster_news
            return cluster_news()
            
        elif stage == 'reports':
            from app.jobs.reports_job import generate_reports
            return generate_reports()
            
        elif stage == 'social_media':
            from app.jobs.social_media_job import generate_social_media_content
            return generate_social_media_content()
            
        elif stage == 'images':
            from app.jobs.image_generation_job import generate_images
            return generate_images()
            
        elif stage == 'audio':
            from app.jobs.audio_generation_job import generate_audio
            return generate_audio()
            
        else:
            return {'error': f'Unknown stage: {stage}'}
            
    except Exception as e:
        return {'error': str(e)}


if __name__ == "__main__":
    run_processing_pipeline()