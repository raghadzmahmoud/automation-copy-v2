#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ•·ï¸ News Crawler
Ø§Ù„Ø²Ø­Ù Ø¹Ù„Ù‰ ØµÙØ­Ø§Øª Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† ÙƒÙ„ ØµÙØ­Ø©
ÙŠØ³ØªØ®Ø¯Ù… SmartScraper Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØµÙØ­Ø§Øª
"""

import time
from typing import List, Optional
from dataclasses import dataclass, field
from urllib.parse import urlparse

from app.services.ingestion.smart_scraper import (
    SmartScraper, 
    ScrapeResult, 
    PageType, 
    ContentStatus
)


@dataclass
class NewsArticle:
    """Ø®Ø¨Ø± Ù…Ø³Ø­ÙˆØ¨"""
    url: str
    title: str
    content: str
    image: str = ""
    published_date: Optional[str] = None
    author: Optional[str] = None


@dataclass 
class CrawlResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø²Ø­Ù"""
    success: bool
    base_url: str
    domain: str
    
    articles: List[NewsArticle] = field(default_factory=list)
    total_links_found: int = 0
    total_articles_scraped: int = 0
    
    combined_content: str = ""
    all_images: List[str] = field(default_factory=list)
    
    error_message: Optional[str] = None
    failed_urls: List[str] = field(default_factory=list)
    
    crawl_time_seconds: float = 0.0


class NewsCrawler:
    """
    ğŸ•·ï¸ News Crawler
    ÙŠØ²Ø­Ù Ø¹Ù„Ù‰ ØµÙØ­Ø© Ø£Ø®Ø¨Ø§Ø± ÙˆÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† ÙƒÙ„ Ø®Ø¨Ø±
    """
    
    def __init__(
        self,
        max_articles: int = 10,
        timeout: int = 30,
        delay_between_requests: float = 1.0
    ):
        self.max_articles = max_articles
        self.timeout = timeout
        self.delay = delay_between_requests
        
        # SmartScraper Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØµÙØ­Ø§Øª
        self.scraper = SmartScraper(timeout=timeout)
    
    def crawl(self, url: str) -> CrawlResult:
        """Ø§Ù„Ø²Ø­Ù Ø¹Ù„Ù‰ ØµÙØ­Ø© Ø£Ø®Ø¨Ø§Ø±"""
        start_time = time.time()
        domain = self._get_domain(url)
        
        print(f"\nğŸ•·ï¸ Starting crawl: {url}")
        
        try:
            # Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø³Ø­Ø¨ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            print(f"   ğŸ“„ Fetching main page...")
            main_result = self.scraper.scrape(url)
            
            # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©
            if main_result.page_type == PageType.LISTING or main_result.article_links:
                # ØµÙØ­Ø© Ù‚ÙˆØ§Ø¦Ù… - Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ø²Ø­Ù Ø¹Ù„ÙŠÙ‡Ø§
                links = main_result.article_links
                if not links:
                    # Ø¬Ø±Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø©
                    links = self._extract_links_from_page(url)
                
                return self._crawl_from_links(
                    url, domain, links, start_time
                )
            
            elif main_result.content_status == ContentStatus.FULL:
                # ØµÙØ­Ø© Ù…Ù‚Ø§Ù„ ÙˆØ§Ø­Ø¯ - Ø£Ø±Ø¬Ø¹Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©
                print(f"   ğŸ“° Single article page detected")
                article = NewsArticle(
                    url=url,
                    title=main_result.title,
                    content=main_result.clean_text,
                    image=main_result.images[0] if main_result.images else "",
                    published_date=main_result.published_date,
                    author=main_result.author
                )
                
                return CrawlResult(
                    success=True,
                    base_url=url,
                    domain=domain,
                    articles=[article],
                    total_links_found=0,
                    total_articles_scraped=1,
                    combined_content=f"# {article.title}\n\n{article.content}",
                    all_images=main_result.images,
                    crawl_time_seconds=time.time() - start_time
                )
            
            else:
                # Ù…Ø­ØªÙˆÙ‰ ÙØ§Ø±Øº Ø£Ùˆ Ø¬Ø²Ø¦ÙŠ - Ø¬Ø±Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø·
                print(f"   âš ï¸ Content insufficient, extracting links...")
                links = self._extract_links_from_page(url)
                
                if links:
                    return self._crawl_from_links(url, domain, links, start_time)
                
                return CrawlResult(
                    success=False,
                    base_url=url,
                    domain=domain,
                    error_message="Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø£Ùˆ Ø±ÙˆØ§Ø¨Ø·",
                    crawl_time_seconds=time.time() - start_time
                )
            
        except Exception as e:
            return CrawlResult(
                success=False,
                base_url=url,
                domain=domain,
                error_message=str(e),
                crawl_time_seconds=time.time() - start_time
            )
        finally:
            self.scraper._close_browser()
    
    def _extract_links_from_page(self, url: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† ØµÙØ­Ø©"""
        try:
            from bs4 import BeautifulSoup
            
            resp = self.scraper.session.get(url, timeout=self.timeout, verify=False)
            soup = BeautifulSoup(resp.text, 'html.parser')
            return self.scraper._extract_article_links(soup, url)
        except:
            return []
    
    def _crawl_from_links(
        self, 
        base_url: str, 
        domain: str, 
        links: List[str],
        start_time: float
    ) -> CrawlResult:
        """Ø§Ù„Ø²Ø­Ù Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø·"""
        
        print(f"   ğŸ”— Found {len(links)} article links")
        
        if not links:
            return CrawlResult(
                success=False,
                base_url=base_url,
                domain=domain,
                error_message="Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ù…Ù‚Ø§Ù„Ø§Øª",
                crawl_time_seconds=time.time() - start_time
            )
        
        # Ø³Ø­Ø¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        articles = []
        failed_urls = []
        all_images = []
        
        links_to_process = links[:self.max_articles]
        print(f"   ğŸ“¥ Scraping {len(links_to_process)} articles...")
        
        for i, link in enumerate(links_to_process, 1):
            try:
                print(f"   [{i}/{len(links_to_process)}] {link[:55]}...")
                
                result = self.scraper.scrape(link)
                
                if result.success and result.content_status != ContentStatus.EMPTY:
                    article = NewsArticle(
                        url=link,
                        title=result.title,
                        content=result.clean_text,
                        image=result.images[0] if result.images else "",
                        published_date=result.published_date,
                        author=result.author
                    )
                    articles.append(article)
                    all_images.extend(result.images)
                    
                    method = "ğŸŒ" if result.method_used == "playwright" else "ğŸ“„"
                    print(f"       {method} âœ… {result.title[:35]}... ({len(result.clean_text)} chars)")
                else:
                    failed_urls.append(link)
                    print(f"       âš ï¸ {result.error_message or 'No content'}")
                
                if i < len(links_to_process):
                    time.sleep(self.delay)
                    
            except Exception as e:
                failed_urls.append(link)
                print(f"       âŒ Error: {str(e)[:30]}")
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        combined_content = self._combine_articles(articles)
        
        crawl_time = time.time() - start_time
        
        print(f"\n   âœ… Crawl complete!")
        print(f"   ğŸ“Š Articles: {len(articles)}/{len(links_to_process)}")
        print(f"   â±ï¸ Time: {crawl_time:.2f}s")
        
        return CrawlResult(
            success=len(articles) > 0,
            base_url=base_url,
            domain=domain,
            articles=articles,
            total_links_found=len(links),
            total_articles_scraped=len(articles),
            combined_content=combined_content,
            all_images=all_images[:20],
            failed_urls=failed_urls,
            crawl_time_seconds=crawl_time
        )
    
    def _combine_articles(self, articles: List[NewsArticle]) -> str:
        """Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ÙÙŠ Ù†Øµ ÙˆØ§Ø­Ø¯"""
        parts = []
        
        for i, article in enumerate(articles, 1):
            part = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“° Ø®Ø¨Ø± Ø±Ù‚Ù… {i}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {article.title}
Ø§Ù„Ø±Ø§Ø¨Ø·: {article.url}
{'Ø§Ù„ØªØ§Ø±ÙŠØ®: ' + article.published_date if article.published_date else ''}

Ø§Ù„Ù…Ø­ØªÙˆÙ‰:
{article.content}
"""
            parts.append(part)
        
        return '\n'.join(parts)
    
    def _get_domain(self, url: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†"""
        try:
            return urlparse(url).netloc
        except:
            return ""


# ============================================
# ğŸš€ Helper Function
# ============================================

def crawl_news_site(url: str, max_articles: int = 10) -> CrawlResult:
    """Ø¯Ø§Ù„Ø© Ù…Ø®ØªØµØ±Ø© Ù„Ù„Ø²Ø­Ù"""
    crawler = NewsCrawler(max_articles=max_articles)
    return crawler.crawl(url)