#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ðŸ“¥ Manual URL Scraper
Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø±Ø§Ø¨Ø· ÙŠØ¯Ø®Ù„Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ø§Ù„Ù€ Database
"""

import time
from typing import List, Dict, Optional
from dataclasses import dataclass, field
from datetime import datetime, timezone
from urllib.parse import urlparse

# Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
from app.services.ingestion.source_detector import SourceDetector, SourceType, SourceInfo
from app.services.ingestion.smart_scraper import SmartScraper, PageType, ContentStatus
from app.services.ingestion.news_crawler import NewsCrawler, CrawlResult
from app.services.ingestion.content_extractor import (
    ContentExtractor, 
    extract_and_prepare_news, 
    ExtractionResult
)

# Database
from app.utils.database import get_db_connection


# ============================================
# ðŸ—„ï¸ Database Functions
# ============================================

def get_or_create_source(url: str) -> int:
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ source_id Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø­Ø¯ Ø¬Ø¯ÙŠØ¯
    
    - ÙŠØ¨Ø­Ø« Ø£ÙˆÙ„Ø§Ù‹ Ø¹Ù† source Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ù†ÙØ³ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
    - Ø¥Ø°Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ ÙŠÙ†Ø´Ø¦ ÙˆØ§Ø­Ø¯ Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ source_type_id = 3 (URL Scrape)
    - Ø§Ù„Ù€ id ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ù€ Database (SERIAL)
    """
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
    parsed = urlparse(url)
    domain = parsed.netloc.replace('www.', '')  # Ø¥Ø²Ø§Ù„Ø© www
    
    conn = get_db_connection()
    cur = conn.cursor()
    
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† source Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ù†ÙØ³ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
        cur.execute("""
            SELECT id, name FROM sources 
            WHERE url LIKE %s OR url LIKE %s OR name LIKE %s
            LIMIT 1
        """, (f'%{domain}%', f'%www.{domain}%', f'%{domain}%'))
        
        result = cur.fetchone()
        
        if result:
            print(f"   âœ… Found existing source: {result[1]} (id={result[0]})")
            return result[0]
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù€ sequence Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        cur.execute("SELECT setval('sources_id_seq', (SELECT COALESCE(MAX(id), 0) FROM sources))")
        
        # Ø¥Ù†Ø´Ø§Ø¡ source Ø¬Ø¯ÙŠØ¯
        # source_type_id = 3 â†’ URL Scrape (Ù…Ù† Ø¬Ø¯ÙˆÙ„ source_types)
        cur.execute("""
            INSERT INTO sources (name, source_type_id, url, is_active, created_at, updated_at)
            VALUES (%s, %s, %s, %s, NOW(), NOW())
            RETURNING id
        """, (
            domain,           # name = Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
            3,                # source_type_id = 3 (URL Scrape)
            url,              # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„ÙƒØ§Ù…Ù„
            True              # is_active
        ))
        
        source_id = cur.fetchone()[0]
        conn.commit()
        
        print(f"   ðŸ†• Created new source: {domain} (id={source_id}, type=URL Scrape)")
        return source_id
        
    except Exception as e:
        conn.rollback()
        print(f"   âš ï¸ Error with source: {e}")
        raise e
    finally:
        cur.close()
        conn.close()


def get_or_create_category_id(category_name: str) -> int:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ category_id Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø­Ø¯ Ø¬Ø¯ÙŠØ¯"""
    conn = get_db_connection()
    cur = conn.cursor()
    
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† category Ù…ÙˆØ¬ÙˆØ¯
        cur.execute("SELECT id FROM categories WHERE name = %s", (category_name,))
        result = cur.fetchone()
        
        if result:
            return result[0]
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù€ sequence Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        cur.execute("SELECT setval('categories_id_seq', (SELECT COALESCE(MAX(id), 0) FROM categories))")
        
        # Ø¥Ù†Ø´Ø§Ø¡ category Ø¬Ø¯ÙŠØ¯
        cur.execute("""
            INSERT INTO categories (name, created_at, updated_at)
            VALUES (%s, NOW(), NOW())
            RETURNING id
        """, (category_name,))
        
        category_id = cur.fetchone()[0]
        conn.commit()
        print(f"   ðŸ†• Created new category: {category_name} (id={category_id})")
        return category_id
        
    except Exception as e:
        conn.rollback()
        return 1  # default category
    finally:
        cur.close()
        conn.close()


def get_input_method_id() -> int:
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ input_method_id Ù„Ù€ URL Scrape
    ÙŠØ¨Ø­Ø« Ø¹Ù† "URL Scrape" Ø£Ùˆ "url_scrape" Ø£Ùˆ ÙŠÙ†Ø´Ø¦ ÙˆØ§Ø­Ø¯ Ø¬Ø¯ÙŠØ¯
    """
    conn = get_db_connection()
    cur = conn.cursor()
    
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† input method Ù…ÙˆØ¬ÙˆØ¯
        cur.execute("""
            SELECT id FROM input_methods 
            WHERE LOWER(name) LIKE '%url%' OR LOWER(name) LIKE '%scrape%'
            LIMIT 1
        """)
        result = cur.fetchone()
        
        if result:
            return result[0]
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù€ sequence Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        cur.execute("SELECT setval('input_methods_id_seq', (SELECT COALESCE(MAX(id), 0) FROM input_methods))")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
        cur.execute("""
            INSERT INTO input_methods (name, description, category, is_active, created_at, updated_at)
            VALUES (%s, %s, %s, %s, NOW(), NOW())
            RETURNING id
        """, (
            'URL Scrape',
            'Manual URL scraping by user',
            'manual',
            True
        ))
        
        method_id = cur.fetchone()[0]
        conn.commit()
        print(f"   ðŸ†• Created input method: URL Scrape (id={method_id})")
        return method_id
        
    except Exception as e:
        conn.rollback()
        return 1
    finally:
        cur.close()
        conn.close()


def save_news_to_db(news_list: List[Dict], source_url: str) -> int:
    """
    Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙŠ raw_news
    
    Returns:
        int: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
    """
    if not news_list:
        return 0
    
    conn = get_db_connection()
    cur = conn.cursor()
    
    saved_count = 0
    
    try:
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù€ sequence Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        cur.execute("SELECT setval('raw_news_id_seq', (SELECT COALESCE(MAX(id), 0) FROM raw_news))")
        
        for news in news_list:
            try:
                cur.execute("""
                    INSERT INTO raw_news (
                        title, content_text, content_img, content_video,
                        tags, source_id, language_id, category_id,
                        input_method_id, source_url, published_at, collected_at
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW()
                    )
                """, (
                    news.get('title', ''),
                    news.get('content_text', ''),
                    news.get('content_img', ''),
                    news.get('content_video', ''),
                    news.get('tags', ''),
                    news.get('source_id'),
                    news.get('language_id', 1),  # 1 = Ø¹Ø±Ø¨ÙŠ
                    news.get('category_id'),
                    news.get('input_method_id'),
                    source_url
                ))
                saved_count += 1
                
            except Exception as e:
                print(f"   âš ï¸ Error saving news '{news.get('title', '')[:30]}...': {str(e)[:50]}")
                continue
        
        conn.commit()
        return saved_count
        
    except Exception as e:
        conn.rollback()
        print(f"   âŒ Database error: {e}")
        return 0
    finally:
        cur.close()
        conn.close()


# ============================================
# ðŸ“¥ Manual Scraper
# ============================================

@dataclass
class ManualScrapeResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„ÙŠØ¯ÙˆÙŠ"""
    success: bool
    url: str
    source_type: str
    scrape_mode: str = "single"
    
    # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    news_extracted: int = 0
    news_saved: int = 0
    pages_crawled: int = 0
    source_id: int = 0
    
    # Ø§Ù„ØªÙØ§ØµÙŠÙ„
    news_items: List[Dict] = field(default_factory=list)
    
    # Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    error_message: Optional[str] = None
    errors: List[str] = field(default_factory=list)
    
    # Ø§Ù„ØªÙˆÙ‚ÙŠØª
    processing_time_seconds: float = 0.0


class ManualScraper:
    """
    ðŸ“¥ Manual Scraper
    Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ø§Ù„Ù€ Database
    
    Usage:
        scraper = ManualScraper()
        result = scraper.scrape_url("https://www.maannews.net")
        
        if result.success:
            print(f"Saved {result.news_saved} news to source_id={result.source_id}")
    """
    
    def __init__(
        self,
        default_language_id: int = 1,
        auto_save: bool = True,
        timeout: int = 30,
        max_articles: int = 10
    ):
        """
        Args:
            default_language_id: ID Ø§Ù„Ù„ØºØ© (1 = Ø¹Ø±Ø¨ÙŠ)
            auto_save: Ø­ÙØ¸ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙÙŠ DB
            timeout: Ù…Ù‡Ù„Ø© Ø§Ù„Ø³Ø­Ø¨
            max_articles: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø£Ø®Ø¨Ø§Ø±
        """
        self.default_language_id = default_language_id
        self.auto_save = auto_save
        self.timeout = timeout
        self.max_articles = max_articles
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self.detector = SourceDetector()
        self.scraper = SmartScraper(timeout=timeout)
        self.news_crawler = NewsCrawler(max_articles=max_articles, timeout=timeout)
        self.extractor = ContentExtractor()
    
    def scrape_url(
        self,
        url: str,
        language_id: Optional[int] = None,
        save_to_db: Optional[bool] = None,
        force_crawl: bool = False,
        force_single: bool = False
    ) -> ManualScrapeResult:
        """
        Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø±Ø§Ø¨Ø·
        
        Args:
            url: Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø±Ø§Ø¯ Ø³Ø­Ø¨Ù‡
            language_id: ID Ø§Ù„Ù„ØºØ© (1 = Ø¹Ø±Ø¨ÙŠ)
            save_to_db: Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù€ Database
            force_crawl: Ø¥Ø¬Ø¨Ø§Ø± ÙˆØ¶Ø¹ Ø§Ù„Ø²Ø­Ù
            force_single: Ø¥Ø¬Ø¨Ø§Ø± ÙˆØ¶Ø¹ Ø§Ù„ØµÙØ­Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©
        
        Returns:
            ManualScrapeResult: Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø³Ø­Ø¨
        """
        start_time = time.time()
        
        language_id = language_id or self.default_language_id
        should_save = save_to_db if save_to_db is not None else self.auto_save
        
        print(f"\n{'='*60}")
        print(f"ðŸ“¥ Manual URL Scraper")
        print(f"{'='*60}")
        print(f"ðŸ”— URL: {url}")
        print(f"ðŸ’¾ Auto Save: {should_save}")
        
        # ===== Step 1: Source Detection =====
        print(f"\nðŸ” Step 1: Detecting source type...")
        source_info = self.detector.detect(url)
        
        if not source_info.is_valid:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type="unknown",
                error_message=source_info.error_message or "Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­",
                processing_time_seconds=time.time() - start_time
            )
        
        print(f"   âœ… Type: {source_info.source_type.value}")
        print(f"   ðŸ“ Domain: {source_info.domain}")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Telegram
        if source_info.source_type in [SourceType.TELEGRAM_CHANNEL, SourceType.TELEGRAM_POST]:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type=source_info.source_type.value,
                error_message="Telegram scraping not implemented yet",
                processing_time_seconds=time.time() - start_time
            )
        
        # ===== Step 2: Get/Create Source & Input Method =====
        print(f"\nðŸ“Œ Step 2: Getting/Creating source in database...")
        
        try:
            source_id = get_or_create_source(source_info.normalized_url)
            input_method_id = get_input_method_id()
            print(f"   âœ… Source ID: {source_id}")
            print(f"   âœ… Input Method ID: {input_method_id}")
        except Exception as e:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type=source_info.source_type.value,
                error_message=f"Database error: {str(e)}",
                processing_time_seconds=time.time() - start_time
            )
        
        # ===== Step 3: ØªØ­Ø¯ÙŠØ¯ ÙˆØ¶Ø¹ Ø§Ù„Ø³Ø­Ø¨ =====
        use_crawl = force_crawl or (self._should_crawl(url) and not force_single)
        mode = "crawl" if use_crawl else "single"
        print(f"\nðŸ“‹ Step 3: Scrape mode: {mode.upper()}")
        
        # ===== Step 4: Ø§Ù„Ø³Ø­Ø¨ =====
        if use_crawl:
            result = self._scrape_with_crawl(
                url=source_info.normalized_url,
                source_id=source_id,
                language_id=language_id,
                input_method_id=input_method_id,
                should_save=should_save,
                start_time=start_time
            )
        else:
            result = self._scrape_single_page(
                url=source_info.normalized_url,
                source_info=source_info,
                source_id=source_id,
                language_id=language_id,
                input_method_id=input_method_id,
                should_save=should_save,
                start_time=start_time
            )
        
        result.scrape_mode = mode
        result.source_id = source_id
        return result
    
    def _should_crawl(self, url: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ø¥Ø°Ø§ Ù†Ø­ØªØ§Ø¬ crawling"""
        import re
        
        # ØµÙØ­Ø© Ø±Ø¦ÙŠØ³ÙŠØ© (domain ÙÙ‚Ø·)
        if re.match(r'^https?://[^/]+/?$', url):
            return True
        
        # Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ID Ø£Ùˆ ØªØ§Ø±ÙŠØ® = Ù„ÙŠØ³ Ù…Ù‚Ø§Ù„
        if not re.search(r'/\d{4}/', url) and not re.search(r'/\d+/?$', url):
            return True
        
        return False
    
    def _scrape_with_crawl(
        self,
        url: str,
        source_id: int,
        language_id: int,
        input_method_id: int,
        should_save: bool,
        start_time: float
    ) -> ManualScrapeResult:
        """Ø§Ù„Ø³Ø­Ø¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø²Ø­Ù"""
        
        print(f"\nðŸ•·ï¸ Step 4: Crawling news site...")
        
        crawl_result = self.news_crawler.crawl(url)
        
        if not crawl_result.success or not crawl_result.combined_content:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type="web",
                pages_crawled=crawl_result.total_articles_scraped,
                error_message=crawl_result.error_message or "ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø²Ø­Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹",
                processing_time_seconds=time.time() - start_time
            )
        
        print(f"   âœ… Crawled {crawl_result.total_articles_scraped} articles")
        print(f"   ðŸ“ Combined content: {len(crawl_result.combined_content)} chars")
        
        # ===== Step 5: LLM Extraction =====
        print(f"\nðŸ¤– Step 5: Extracting news with AI...")
        
        news_list = extract_and_prepare_news(
            content=crawl_result.combined_content,
            source_url=url,
            source_id=source_id,
            language_id=language_id,
            available_images=crawl_result.all_images
        )
        
        if not news_list:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type="web",
                pages_crawled=crawl_result.total_articles_scraped,
                error_message="Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
                processing_time_seconds=time.time() - start_time
            )
        
        # Ø¥Ø¶Ø§ÙØ© input_method_id Ù„ÙƒÙ„ Ø®Ø¨Ø±
        for news in news_list:
            news['input_method_id'] = input_method_id
        
        return self._finalize_and_save(
            news_list=news_list,
            url=url,
            source_type="web",
            source_id=source_id,
            should_save=should_save,
            start_time=start_time,
            pages_crawled=crawl_result.total_articles_scraped
        )
    
    def _scrape_single_page(
        self,
        url: str,
        source_info: SourceInfo,
        source_id: int,
        language_id: int,
        input_method_id: int,
        should_save: bool,
        start_time: float
    ) -> ManualScrapeResult:
        """Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©"""
        
        print(f"\nðŸŒ Step 4: Scraping single page...")
        
        scraped = self.scraper.scrape(url)
        
        if not scraped.success:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type=source_info.source_type.value,
                error_message=scraped.error_message or "ÙØ´Ù„ ÙÙŠ Ø³Ø­Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
                processing_time_seconds=time.time() - start_time
            )
        
        print(f"   âœ… Title: {scraped.title[:50]}..." if scraped.title else "   âš ï¸ No title")
        print(f"   ðŸ“ Content: {len(scraped.clean_text)} chars")
        print(f"   ðŸ“„ Page Type: {scraped.page_type.value}")
        print(f"   ðŸ”§ Method: {scraped.method_used}")
        
        # Ø¥Ø°Ø§ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù‚ØµÙŠØ±ØŒ Ø¬Ø±Ø¨ Ø§Ù„Ø²Ø­Ù
        if scraped.content_status in [ContentStatus.EMPTY, ContentStatus.PARTIAL]:
            print(f"   âš ï¸ Content insufficient, switching to crawl mode...")
            return self._scrape_with_crawl(
                url=url,
                source_id=source_id,
                language_id=language_id,
                input_method_id=input_method_id,
                should_save=should_save,
                start_time=start_time
            )
        
        # ===== Step 5: LLM Extraction =====
        print(f"\nðŸ¤– Step 5: Extracting news with AI...")
        
        news_list = extract_and_prepare_news(
            content=scraped.clean_text,
            source_url=url,
            source_id=source_id,
            language_id=language_id,
            available_images=scraped.images
        )
        
        if not news_list:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type=source_info.source_type.value,
                error_message="Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
                processing_time_seconds=time.time() - start_time
            )
        
        # Ø¥Ø¶Ø§ÙØ© input_method_id Ù„ÙƒÙ„ Ø®Ø¨Ø±
        for news in news_list:
            news['input_method_id'] = input_method_id
        
        return self._finalize_and_save(
            news_list=news_list,
            url=url,
            source_type=source_info.source_type.value,
            source_id=source_id,
            should_save=should_save,
            start_time=start_time
        )
    
    def _finalize_and_save(
        self,
        news_list: List[Dict],
        url: str,
        source_type: str,
        source_id: int,
        should_save: bool,
        start_time: float,
        pages_crawled: int = 0
    ) -> ManualScrapeResult:
        """ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª ÙˆØ§Ù„Ø­ÙØ¸"""
        
        # ===== ØªØ¹ÙŠÙŠÙ† category_id =====
        print(f"\nðŸ“ Step 6: Assigning categories...")
        
        for news in news_list:
            if 'category_name' in news:
                news['category_id'] = get_or_create_category_id(news['category_name'])
                del news['category_name']
        
        print(f"   âœ… Processed {len(news_list)} news items")
        
        # ===== Save to Database =====
        saved_count = 0
        
        if should_save:
            print(f"\nðŸ’¾ Step 7: Saving to database (raw_news)...")
            saved_count = save_news_to_db(news_list, url)
            print(f"   âœ… Saved {saved_count}/{len(news_list)} news items")
        else:
            print(f"\nâ­ï¸ Step 7: Skipping database save (auto_save=False)")
        
        # ===== Ø§Ù„Ù†ØªÙŠØ¬Ø© =====
        processing_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print(f"âœ… Completed in {processing_time:.2f}s")
        print(f"   ðŸ“° Extracted: {len(news_list)}")
        print(f"   ðŸ’¾ Saved: {saved_count}")
        print(f"   ðŸ”— Source ID: {source_id}")
        if pages_crawled:
            print(f"   ðŸ•·ï¸ Pages crawled: {pages_crawled}")
        print(f"{'='*60}\n")
        
        return ManualScrapeResult(
            success=True,
            url=url,
            source_type=source_type,
            news_extracted=len(news_list),
            news_saved=saved_count,
            pages_crawled=pages_crawled,
            source_id=source_id,
            news_items=news_list,
            processing_time_seconds=processing_time
        )


# ============================================
# ðŸš€ Ø¯ÙˆØ§Ù„ Ù…Ø®ØªØµØ±Ø©
# ============================================

def scrape_url(url: str, save_to_db: bool = True, max_articles: int = 10) -> ManualScrapeResult:
    """Ø¯Ø§Ù„Ø© Ù…Ø®ØªØµØ±Ø© Ù„Ø³Ø­Ø¨ Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯"""
    scraper = ManualScraper(auto_save=save_to_db, max_articles=max_articles)
    return scraper.scrape_url(url)


def scrape_urls(urls: List[str], save_to_db: bool = True, max_articles: int = 10) -> List[ManualScrapeResult]:
    """Ø¯Ø§Ù„Ø© Ù…Ø®ØªØµØ±Ø© Ù„Ø³Ø­Ø¨ Ø¹Ø¯Ø© Ø±ÙˆØ§Ø¨Ø·"""
    scraper = ManualScraper(auto_save=save_to_db, max_articles=max_articles)
    results = []
    
    for i, url in enumerate(urls, 1):
        print(f"\nðŸ“Œ [{i}/{len(urls)}] Processing: {url[:50]}...")
        result = scraper.scrape_url(url)
        results.append(result)
        
        if i < len(urls):
            time.sleep(3)
    
    return results