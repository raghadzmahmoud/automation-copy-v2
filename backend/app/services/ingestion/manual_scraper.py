#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ“¥ Manual URL Scraper
Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø±Ø§Ø¨Ø· ÙŠØ¯Ø®Ù„Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
ÙŠØ¬Ù…Ø¹: Source Detection + Web Scraping + LLM Extraction
"""

import time
from typing import List, Dict, Optional
from dataclasses import dataclass, field
from datetime import datetime, timezone

# Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª - imports Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
from app.services.ingestion.source_detector import SourceDetector, SourceType, SourceInfo
from app.services.ingestion.web_scraper import WebScraper, ScrapedContent
from app.services.ingestion.content_extractor import (
    ContentExtractor, 
    extract_and_prepare_news, 
    ExtractionResult
)

# Database utilities
try:
    from app.utils.database import (
        get_or_create_category_id,
        save_news_batch,
        get_db_connection
    )
    DB_AVAILABLE = True
except ImportError:
    DB_AVAILABLE = False
    print("âš ï¸ Database module not available - running in standalone mode")


@dataclass
class ManualScrapeResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„ÙŠØ¯ÙˆÙŠ"""
    success: bool
    url: str
    source_type: str
    
    # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    news_extracted: int = 0
    news_saved: int = 0
    
    # Ø§Ù„ØªÙØ§ØµÙŠÙ„
    news_items: List[Dict] = field(default_factory=list)
    scraped_content: Optional[ScrapedContent] = None
    
    # Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    error_message: Optional[str] = None
    errors: List[str] = field(default_factory=list)
    
    # Ø§Ù„ØªÙˆÙ‚ÙŠØª
    processing_time_seconds: float = 0.0


class ManualScraper:
    """
    ğŸ“¥ Manual Scraper
    Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ¯Ø®Ù„Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    
    Usage:
        scraper = ManualScraper()
        result = scraper.scrape_url("https://example.com/news")
        
        if result.success:
            print(f"Extracted {result.news_extracted} news")
            print(f"Saved {result.news_saved} to database")
    """
    
    def __init__(
        self,
        default_source_id: int = 1,
        default_language_id: int = 1,
        auto_save: bool = True,
        timeout: int = 30
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù€ Scraper
        
        Args:
            default_source_id: ID Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
            default_language_id: ID Ø§Ù„Ù„ØºØ© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© (1 = Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
            auto_save: Ø­ÙØ¸ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙÙŠ DB
            timeout: Ù…Ù‡Ù„Ø© Ø§Ù„Ø³Ø­Ø¨
        """
        self.default_source_id = default_source_id
        self.default_language_id = default_language_id
        self.auto_save = auto_save and DB_AVAILABLE
        self.timeout = timeout
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self.detector = SourceDetector()
        self.web_scraper = WebScraper(timeout=timeout)
        self.extractor = ContentExtractor()
    
    def scrape_url(
        self,
        url: str,
        source_id: Optional[int] = None,
        language_id: Optional[int] = None,
        save_to_db: Optional[bool] = None
    ) -> ManualScrapeResult:
        """
        Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø±Ø§Ø¨Ø·
        
        Args:
            url: Ø§Ù„Ø±Ø§Ø¨Ø·
            source_id: ID Ø§Ù„Ù…ØµØ¯Ø± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            language_id: ID Ø§Ù„Ù„ØºØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            save_to_db: Ø­ÙØ¸ ÙÙŠ DB (None = Ø§Ø³ØªØ®Ø¯Ø§Ù… auto_save)
        
        Returns:
            ManualScrapeResult: Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø³Ø­Ø¨
        """
        start_time = time.time()
        errors = []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        source_id = source_id or self.default_source_id
        language_id = language_id or self.default_language_id
        should_save = save_to_db if save_to_db is not None else self.auto_save
        
        print(f"\n{'='*60}")
        print(f"ğŸ“¥ Manual Scraper")
        print(f"{'='*60}")
        print(f"ğŸ”— URL: {url}")
        
        # ===== Step 1: Source Detection =====
        print(f"\nğŸ” Step 1: Detecting source type...")
        source_info = self.detector.detect(url)
        
        if not source_info.is_valid:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type="unknown",
                error_message=source_info.error_message or "Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­",
                processing_time_seconds=time.time() - start_time
            )
        
        print(f"   âœ… Type: {source_info.source_type.value}")
        print(f"   ğŸ“ Domain: {source_info.domain}")
        
        # ===== Step 2: Content Scraping =====
        print(f"\nğŸŒ Step 2: Scraping content...")
        
        if source_info.source_type == SourceType.WEB:
            scraped = self.web_scraper.scrape(source_info.normalized_url)
        elif source_info.source_type == SourceType.RSS:
            # TODO: Ø§Ø³ØªØ®Ø¯Ø§Ù… RSS scraper Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
            errors.append("RSS scraping not implemented in manual mode yet")
            scraped = self.web_scraper.scrape(source_info.normalized_url)
        elif source_info.source_type in [SourceType.TELEGRAM_CHANNEL, SourceType.TELEGRAM_POST]:
            # TODO: Telegram scraper
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type=source_info.source_type.value,
                error_message="Telegram scraping not implemented yet",
                processing_time_seconds=time.time() - start_time
            )
        else:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type="unknown",
                error_message="Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…",
                processing_time_seconds=time.time() - start_time
            )
        
        if not scraped.success:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type=source_info.source_type.value,
                error_message=scraped.error_message or "ÙØ´Ù„ ÙÙŠ Ø³Ø­Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
                processing_time_seconds=time.time() - start_time
            )
        
        print(f"   âœ… Title: {scraped.title[:50]}..." if scraped.title else "   âš ï¸ No title")
        print(f"   ğŸ“ Content: {len(scraped.clean_text)} chars")
        print(f"   ğŸ–¼ï¸ Images: {len(scraped.images)}")
        
        # ===== Step 3: LLM Extraction =====
        print(f"\nğŸ¤– Step 3: Extracting news with AI...")
        
        news_list = extract_and_prepare_news(
            content=scraped.clean_text,
            source_url=source_info.normalized_url,
            source_id=source_id,
            language_id=language_id,
            available_images=scraped.images
        )
        
        if not news_list:
            return ManualScrapeResult(
                success=False,
                url=url,
                source_type=source_info.source_type.value,
                scraped_content=scraped,
                error_message="Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
                processing_time_seconds=time.time() - start_time
            )
        
        print(f"   âœ… Extracted {len(news_list)} news items")
        
        # ===== Step 4: ØªØ¹ÙŠÙŠÙ† category_id =====
        print(f"\nğŸ“ Step 4: Assigning categories...")
        
        for news in news_list:
            if DB_AVAILABLE and 'category_name' in news:
                news['category_id'] = get_or_create_category_id(news['category_name'])
                del news['category_name']
            elif 'category_name' in news:
                # Standalone mode - keep category_name
                news['category_id'] = 1  # default
        
        # ===== Step 5: Save to Database =====
        saved_count = 0
        
        if should_save and DB_AVAILABLE:
            print(f"\nğŸ’¾ Step 5: Saving to database...")
            saved_count = save_news_batch(news_list)
            print(f"   âœ… Saved {saved_count}/{len(news_list)} news items")
        else:
            print(f"\nâ­ï¸ Step 5: Skipping database save")
            if not DB_AVAILABLE:
                print("   âš ï¸ Database not available")
        
        # ===== Ø§Ù„Ù†ØªÙŠØ¬Ø© =====
        processing_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print(f"âœ… Completed in {processing_time:.2f}s")
        print(f"   ğŸ“° Extracted: {len(news_list)}")
        print(f"   ğŸ’¾ Saved: {saved_count}")
        print(f"{'='*60}\n")
        
        return ManualScrapeResult(
            success=True,
            url=url,
            source_type=source_info.source_type.value,
            news_extracted=len(news_list),
            news_saved=saved_count,
            news_items=news_list,
            scraped_content=scraped,
            errors=errors,
            processing_time_seconds=processing_time
        )
    
    def scrape_multiple(
        self,
        urls: List[str],
        delay_seconds: int = 5
    ) -> List[ManualScrapeResult]:
        """
        Ø³Ø­Ø¨ Ù…Ù† Ø¹Ø¯Ø© Ø±ÙˆØ§Ø¨Ø·
        
        Args:
            urls: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            delay_seconds: Ø§Ù„ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        
        Returns:
            List[ManualScrapeResult]: Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø³Ø­Ø¨
        """
        results = []
        
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ“Œ [{i}/{len(urls)}] Processing...")
            
            result = self.scrape_url(url)
            results.append(result)
            
            if i < len(urls):
                print(f"â³ Waiting {delay_seconds}s before next URL...")
                time.sleep(delay_seconds)
        
        # Ù…Ù„Ø®Øµ
        total_extracted = sum(r.news_extracted for r in results)
        total_saved = sum(r.news_saved for r in results)
        successful = sum(1 for r in results if r.success)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Batch Summary")
        print(f"{'='*60}")
        print(f"   URLs processed: {len(urls)}")
        print(f"   Successful: {successful}")
        print(f"   Failed: {len(urls) - successful}")
        print(f"   Total news extracted: {total_extracted}")
        print(f"   Total news saved: {total_saved}")
        
        return results


# ============================================
# ğŸš€ Ø¯ÙˆØ§Ù„ Ù…Ø®ØªØµØ±Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹
# ============================================

def scrape_url(url: str, save_to_db: bool = True) -> ManualScrapeResult:
    """
    Ø¯Ø§Ù„Ø© Ù…Ø®ØªØµØ±Ø© Ù„Ø³Ø­Ø¨ Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯
    
    Usage:
        from app.services.ingestion.manual_scraper import scrape_url
        
        result = scrape_url("https://example.com/news")
        print(f"Extracted: {result.news_extracted}")
    """
    scraper = ManualScraper(auto_save=save_to_db)
    return scraper.scrape_url(url)


def scrape_urls(urls: List[str], save_to_db: bool = True) -> List[ManualScrapeResult]:
    """
    Ø¯Ø§Ù„Ø© Ù…Ø®ØªØµØ±Ø© Ù„Ø³Ø­Ø¨ Ø¹Ø¯Ø© Ø±ÙˆØ§Ø¨Ø·
    """
    scraper = ManualScraper(auto_save=save_to_db)
    return scraper.scrape_multiple(urls)


# ============================================
# ğŸ§ª Test
# ============================================

if __name__ == "__main__":
    import sys
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø±Ø§Ø¨Ø· ÙÙŠ Ø§Ù„Ø£ÙˆØ§Ù…Ø±
    if len(sys.argv) > 1:
        test_url = sys.argv[1]
    else:
        # Ø±Ø§Ø¨Ø· Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        test_url = "https://www.aljazeera.net"
    
    print(f"\nğŸ§ª Testing Manual Scraper")
    print(f"URL: {test_url}\n")
    
    # ØªØ´ØºÙŠÙ„ Ø¨Ø¯ÙˆÙ† Ø­ÙØ¸ ÙÙŠ DB Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    scraper = ManualScraper(auto_save=False)
    result = scraper.scrape_url(test_url)
    
    if result.success:
        print(f"\nğŸ“° Extracted News:")
        for i, news in enumerate(result.news_items[:3], 1):  # Ø£ÙˆÙ„ 3 ÙÙ‚Ø·
            print(f"\n--- [{i}] ---")
            print(f"ğŸ“Œ {news['title'][:60]}...")
            print(f"ğŸ“ Category ID: {news.get('category_id', 'N/A')}")
            print(f"ğŸ·ï¸ Tags: {news.get('tags', '')[:50]}...")
    else:
        print(f"\nâŒ Error: {result.error_message}")