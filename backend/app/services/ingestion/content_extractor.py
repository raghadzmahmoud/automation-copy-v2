#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ¤– Content Extractor using LLM
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini
"""

import json
import time
import re
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timezone
from google import genai

# Import settings - ÙŠØ¹Ù…Ù„ Ù…Ù† backend/ Ù…Ø¨Ø§Ø´Ø±Ø©
try:
    from settings import GEMINI_API_KEY
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    try:
        from settings import GEMINI_EXTRACTION_MODEL
        EXTRACTION_MODEL = GEMINI_EXTRACTION_MODEL
    except ImportError:
        from settings import GEMINI_MODEL
        EXTRACTION_MODEL = GEMINI_MODEL
except ImportError:
    # Fallback Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¹Ù…Ù„ Ù…Ù† Ù…ÙƒØ§Ù† Ø¢Ø®Ø±
    import os
    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
    EXTRACTION_MODEL = os.getenv('GEMINI_EXTRACTION_MODEL', 'gemini-2.5-flash-lite')


# ØªÙ‡ÙŠØ¦Ø© Gemini client
client = genai.Client(api_key=GEMINI_API_KEY)


# Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„ØµØ§Ù„Ø­Ø© (Ù†ÙØ³ classifier.py)
VALID_CATEGORIES = [
    'Ø³ÙŠØ§Ø³Ø©', 'Ø§Ù‚ØªØµØ§Ø¯', 'Ø±ÙŠØ§Ø¶Ø©', 'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§', 'ØµØ­Ø©',
    'Ø«Ù‚Ø§ÙØ©', 'Ù…Ø­Ù„ÙŠ', 'Ø¯ÙˆÙ„ÙŠ', 'Ø¹Ø³ÙƒØ±ÙŠ', 'Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ', 'ÙÙ†', 'ØªØ¹Ù„ÙŠÙ…'
]


@dataclass
class ExtractedNews:
    """Ø®Ø¨Ø± Ù…Ø³ØªØ®Ø±Ø¬"""
    title: str
    content: str
    category: str
    tags: List[str]
    
    # Ø§Ø®ØªÙŠØ§Ø±ÙŠ
    published_date: Optional[str] = None
    author: Optional[str] = None
    image_url: Optional[str] = None
    
    # Ù„Ù„ØªØ®Ø²ÙŠÙ†
    tags_str: str = ""
    
    def __post_init__(self):
        """ØªØ­ÙˆÙŠÙ„ tags Ù„Ù€ string"""
        if self.tags and not self.tags_str:
            self.tags_str = ", ".join(self.tags)


@dataclass
class ExtractionResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬"""
    success: bool
    news_items: List[ExtractedNews] = field(default_factory=list)
    total_extracted: int = 0
    error_message: Optional[str] = None
    raw_response: Optional[str] = None


class ContentExtractor:
    """
    ğŸ¤– Content Extractor
    ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM
    """
    
    # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ (Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ context window)
    MAX_CONTENT_LENGTH = 30000  # ~30K chars Ù„Ù„Ù†Øµ
    
    def __init__(self, model: str = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
        
        Args:
            model: Ø§Ø³Ù… Ù…ÙˆØ¯ÙŠÙ„ Gemini (Ø§ÙØªØ±Ø§Ø¶ÙŠ: GEMINI_EXTRACTION_MODEL)
        """
        self.model = model or EXTRACTION_MODEL
    
    def extract_news(
        self, 
        content: str, 
        source_url: str = "",
        available_images: List[str] = None,
        max_retries: int = 3
    ) -> ExtractionResult:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        
        Args:
            content: Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù…
            source_url: Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø± (Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„)
            available_images: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ§Ø­Ø©
            max_retries: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
        
        Returns:
            ExtractionResult: Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        """
        if not content or len(content.strip()) < 50:
            return ExtractionResult(
                success=False,
                error_message="Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ ÙØ§Ø±Øº"
            )
        
        # Ø§Ù‚ØªØ·Ø§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
        if len(content) > self.MAX_CONTENT_LENGTH:
            content = content[:self.MAX_CONTENT_LENGTH]
            print(f"   âš ï¸ Content truncated to {self.MAX_CONTENT_LENGTH} chars")
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt
        prompt = self._build_extraction_prompt(content, source_url, available_images)
        
        # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
        for attempt in range(max_retries):
            try:
                # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ LLM
                response = client.models.generate_content(
                    model=self.model,
                    contents=prompt
                )
                
                result_text = response.text.strip()
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¯
                news_items = self._parse_response(result_text, available_images)
                
                if news_items:
                    return ExtractionResult(
                        success=True,
                        news_items=news_items,
                        total_extracted=len(news_items),
                        raw_response=result_text
                    )
                else:
                    raise ValueError("Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ø£Ø®Ø¨Ø§Ø±")
                    
            except json.JSONDecodeError as e:
                print(f"   âš ï¸ Attempt {attempt + 1}: JSON error - {str(e)[:50]}")
                if attempt < max_retries - 1:
                    time.sleep(3)
                    continue
                    
            except Exception as e:
                print(f"   âš ï¸ Attempt {attempt + 1}: {str(e)[:50]}")
                if attempt < max_retries - 1:
                    time.sleep(4)
                    continue
        
        return ExtractionResult(
            success=False,
            error_message=f"ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø¹Ø¯ {max_retries} Ù…Ø­Ø§ÙˆÙ„Ø§Øª"
        )
    
    def _build_extraction_prompt(
        self, 
        content: str, 
        source_url: str,
        available_images: List[str] = None
    ) -> str:
        """Ø¨Ù†Ø§Ø¡ prompt Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬"""
        
        images_section = ""
        if available_images:
            images_list = "\n".join([f"  [{i+1}] {img}" for i, img in enumerate(available_images[:10])])
            images_section = f"""
ğŸ–¼ï¸ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ§Ø­Ø©:
{images_list}

Ø¹Ù†Ø¯ Ø±Ø¨Ø· ØµÙˆØ±Ø© Ø¨Ø®Ø¨Ø±ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù‚Ù… [1], [2], Ø¥Ù„Ø® Ø£Ùˆ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø©.
"""
        
        prompt = f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø£Ø®Ø¨Ø§Ø± Ù…ØªØ®ØµØµ. Ù…Ù‡Ù…ØªÙƒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ§Ù„ÙŠ.

ğŸ“° Ø§Ù„Ù…ØµØ¯Ø±: {source_url}
{images_section}

ğŸ“„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰:
---
{content}
---

ğŸ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
Ø§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„ Ø®Ø¨Ø± Ù…Ù†ÙØµÙ„ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø£Ø¹Ù„Ø§Ù‡. Ù„ÙƒÙ„ Ø®Ø¨Ø± Ø£Ø¹Ø·Ù†ÙŠ:

1. title: Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø®Ø¨Ø± (ÙˆØ§Ø¶Ø­ ÙˆÙ…Ø®ØªØµØ±)
2. content: Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø¨Ø± (Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù‡ØŒ 100-500 ÙƒÙ„Ù…Ø©)
3. category: Ø§Ù„ØªØµÙ†ÙŠÙ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©: {VALID_CATEGORIES}
4. tags: 5-10 ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ø¨Ø¯ÙˆÙ† Ù…Ø³Ø§ÙØ§ØªØŒ Ø§Ø³ØªØ®Ø¯Ù… _ Ù„Ù„Ø±Ø¨Ø·)
5. published_date: ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø± Ø¥Ù† ÙˆØ¬Ø¯ (Ø£Ùˆ null)
6. author: Ø§Ø³Ù… Ø§Ù„ÙƒØ§ØªØ¨ Ø¥Ù† ÙˆØ¬Ø¯ (Ø£Ùˆ null)
7. image_index: Ø±Ù‚Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø£Ø¹Ù„Ø§Ù‡ (Ø£Ùˆ null)

ğŸ“‹ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (JSON ÙÙ‚Ø·):
```json
{{
  "news_count": 3,
  "news_items": [
    {{
      "title": "Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø£ÙˆÙ„",
      "content": "Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ÙƒØ§Ù…Ù„...",
      "category": "Ø³ÙŠØ§Ø³Ø©",
      "tags": ["ÙÙ„Ø³Ø·ÙŠÙ†", "ØºØ²Ø©", "Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„"],
      "published_date": "2024-01-15",
      "author": "Ø§Ø³Ù… Ø§Ù„ÙƒØ§ØªØ¨",
      "image_index": 1
    }},
    {{
      "title": "Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ",
      "content": "...",
      "category": "Ø§Ù‚ØªØµØ§Ø¯",
      "tags": ["..."],
      "published_date": null,
      "author": null,
      "image_index": null
    }}
  ]
}}
```

âš ï¸ Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
- Ø§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„ Ø®Ø¨Ø± Ù…Ù†ÙØµÙ„ (Ù‚Ø¯ ÙŠÙƒÙˆÙ† 1 Ø£Ùˆ 10 Ø£Ùˆ Ø£ÙƒØ«Ø±)
- Ù„Ø§ ØªØ¯Ù…Ø¬ Ø£Ø®Ø¨Ø§Ø± Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø®Ø¨Ø± ÙˆØ§Ø­Ø¯
- Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…ÙÙŠØ¯ ÙˆÙ…ÙÙ‡ÙˆÙ… Ø¨Ø°Ø§ØªÙ‡
- ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠ
- Tags Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù… _ Ø¨Ø¯Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ©
- Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø·ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ

Ø§Ù„Ø±Ø¯:"""
        
        return prompt
    
    def _parse_response(
        self, 
        response_text: str,
        available_images: List[str] = None
    ) -> List[ExtractedNews]:
        """ØªØ­Ù„ÙŠÙ„ Ø±Ø¯ LLM"""
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø¯
        response_text = response_text.replace('```json', '').replace('```', '').strip()
        response_text = response_text.replace('`', '').strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}')
        
        if start_idx == -1 or end_idx == -1:
            raise ValueError("No JSON found in response")
        
        json_str = response_text[start_idx:end_idx+1]
        data = json.loads(json_str)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        news_items = []
        items_data = data.get('news_items', [])
        
        if not items_data:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø®ØªÙ„Ù
            if isinstance(data, list):
                items_data = data
            else:
                raise ValueError("No news_items found")
        
        for item in items_data:
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                title = item.get('title', '').strip()
                content = item.get('content', '').strip()
                category = item.get('category', 'Ù…Ø­Ù„ÙŠ').strip()
                tags = item.get('tags', [])
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                if not title or not content:
                    continue
                
                if len(title) < 5 or len(content) < 30:
                    continue
                
                # ØªØµØ­ÙŠØ­ Ø§Ù„ØªØµÙ†ÙŠÙ
                if category not in VALID_CATEGORIES:
                    category = self._fix_category(category)
                
                # ØªÙ†Ø¸ÙŠÙ Tags
                if isinstance(tags, str):
                    tags = [t.strip() for t in tags.split(',')]
                
                cleaned_tags = self._clean_tags(tags)
                
                # Ø±Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©
                image_url = None
                image_index = item.get('image_index')
                if image_index and available_images:
                    try:
                        idx = int(image_index) - 1  # 1-based to 0-based
                        if 0 <= idx < len(available_images):
                            image_url = available_images[idx]
                    except:
                        pass
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø®Ø¨Ø±
                news = ExtractedNews(
                    title=title,
                    content=content,
                    category=category,
                    tags=cleaned_tags,
                    published_date=item.get('published_date'),
                    author=item.get('author'),
                    image_url=image_url
                )
                
                news_items.append(news)
                
            except Exception as e:
                print(f"   âš ï¸ Error parsing news item: {str(e)[:50]}")
                continue
        
        return news_items
    
    def _fix_category(self, category: str) -> str:
        """Ù…Ø­Ø§ÙˆÙ„Ø© ØªØµØ­ÙŠØ­ Ø§Ù„ØªØµÙ†ÙŠÙ"""
        category_lower = category.lower()
        
        mappings = {
            'Ø³ÙŠØ§Ø³': 'Ø³ÙŠØ§Ø³Ø©',
            'Ø­ÙƒÙˆÙ…': 'Ø³ÙŠØ§Ø³Ø©',
            'Ø§Ù†ØªØ®Ø§Ø¨': 'Ø³ÙŠØ§Ø³Ø©',
            'Ø§Ù‚ØªØµØ§Ø¯': 'Ø§Ù‚ØªØµØ§Ø¯',
            'Ù…Ø§Ù„': 'Ø§Ù‚ØªØµØ§Ø¯',
            'ØªØ¬Ø§Ø±': 'Ø§Ù‚ØªØµØ§Ø¯',
            'Ø±ÙŠØ§Ø¶': 'Ø±ÙŠØ§Ø¶Ø©',
            'ÙƒØ±Ø©': 'Ø±ÙŠØ§Ø¶Ø©',
            'ØªÙ‚Ù†ÙŠ': 'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§',
            'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬': 'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§',
            'ØµØ­': 'ØµØ­Ø©',
            'Ø·Ø¨': 'ØµØ­Ø©',
            'Ø¹Ø³ÙƒØ±': 'Ø¹Ø³ÙƒØ±ÙŠ',
            'Ø¬ÙŠØ´': 'Ø¹Ø³ÙƒØ±ÙŠ',
            'Ø­Ø±Ø¨': 'Ø¹Ø³ÙƒØ±ÙŠ',
            'Ø«Ù‚Ø§Ù': 'Ø«Ù‚Ø§ÙØ©',
            'ÙÙ†': 'ÙÙ†',
            'ØªØ¹Ù„ÙŠÙ…': 'ØªØ¹Ù„ÙŠÙ…',
            'Ø¯ÙˆÙ„': 'Ø¯ÙˆÙ„ÙŠ',
            'Ø¹Ø§Ù„Ù…': 'Ø¯ÙˆÙ„ÙŠ',
        }
        
        for key, value in mappings.items():
            if key in category_lower:
                return value
        
        return 'Ù…Ø­Ù„ÙŠ'  # default
    
    def _clean_tags(self, tags: List) -> List[str]:
        """ØªÙ†Ø¸ÙŠÙ Tags"""
        cleaned = []
        
        for tag in tags[:12]:
            tag = str(tag).strip()
            if not tag:
                continue
            
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
            tag = tag.replace(' ', '_')
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ²
            tag = re.sub(r'[,ØŒ.:\'"ØŸ?!]', '', tag)
            
            if len(tag) > 1:
                cleaned.append(tag)
        
        return cleaned


# ============================================
# ğŸ”— Ø¯Ø§Ù„Ø© Ù„Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
# ============================================

def extract_and_prepare_news(
    content: str,
    source_url: str,
    source_id: int,
    language_id: int = 1,
    available_images: List[str] = None
) -> List[Dict]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØªØ­Ø¶ÙŠØ±Ù‡Ø§ Ù„Ù„Ø­ÙØ¸ ÙÙŠ raw_news
    
    Args:
        content: Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù…
        source_url: Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±
        source_id: ID Ø§Ù„Ù…ØµØ¯Ø± ÙÙŠ DB
        language_id: ID Ø§Ù„Ù„ØºØ©
        available_images: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØµÙˆØ±
    
    Returns:
        List[Dict]: Ù‚Ø§Ø¦Ù…Ø© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø­ÙØ¸ ÙÙŠ raw_news
    """
    extractor = ContentExtractor()
    result = extractor.extract_news(content, source_url, available_images)
    
    if not result.success:
        print(f"   âŒ Extraction failed: {result.error_message}")
        return []
    
    # ØªØ­ÙˆÙŠÙ„ Ù„Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
    news_list = []
    
    for news in result.news_items:
        news_dict = {
            'title': news.title,
            'content_text': news.content,
            'content_img': news.image_url or '',
            'content_video': '',
            'tags': news.tags_str,
            'source_id': source_id,
            'language_id': language_id,
            'category_id': None,  # Ø³ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ†Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
            'category_name': news.category,  # Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹ get_or_create_category_id
            'published_at': _parse_date(news.published_date),
            'collected_at': datetime.now(timezone.utc)
        }
        
        news_list.append(news_dict)
    
    print(f"   âœ… Extracted {len(news_list)} news items")
    return news_list


def _parse_date(date_str: Optional[str]) -> Optional[datetime]:
    """ØªØ­ÙˆÙŠÙ„ string Ù„Ù€ datetime"""
    if not date_str:
        return datetime.now(timezone.utc)
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¹Ø¯Ø© ØªÙ†Ø³ÙŠÙ‚Ø§Øª
        formats = [
            '%Y-%m-%d',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%d %H:%M:%S',
            '%d/%m/%Y',
            '%d-%m-%Y',
        ]
        
        for fmt in formats:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.replace(tzinfo=timezone.utc)
            except:
                continue
        
        return datetime.now(timezone.utc)
        
    except:
        return datetime.now(timezone.utc)


# ============================================
# ğŸ§ª Test
# ============================================

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·
    test_content = """
    Ø¹Ù†ÙˆØ§Ù†: Ø§Ù„Ø­ÙƒÙˆÙ…Ø© ØªØ¹Ù„Ù† Ø¹Ù† Ø®Ø·Ø© Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©
    
    Ø£Ø¹Ù„Ù†Øª Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø§Ù„ÙŠÙˆÙ… Ø¹Ù† Ø®Ø·Ø© Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø´Ø§Ù…Ù„Ø© ØªÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹ÙŠØ´ÙŠ Ù„Ù„Ù…ÙˆØ§Ø·Ù†ÙŠÙ†.
    ÙˆÙ‚Ø§Ù„ ÙˆØ²ÙŠØ± Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙÙŠ Ù…Ø¤ØªÙ…Ø± ØµØ­ÙÙŠ Ø¥Ù† Ø§Ù„Ø®Ø·Ø© ØªØªØ¶Ù…Ù† ØªØ®ÙÙŠØ¶Ø§Øª Ø¶Ø±ÙŠØ¨ÙŠØ© ÙˆØ²ÙŠØ§Ø¯Ø© ÙÙŠ Ø§Ù„Ø±ÙˆØ§ØªØ¨.
    
    ---
    
    Ø±ÙŠØ§Ø¶Ø©: Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ ÙŠÙÙˆØ² Ø¹Ù„Ù‰ Ù†Ø¸ÙŠØ±Ù‡ Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠ
    
    Ø­Ù‚Ù‚ Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… ÙÙˆØ²Ø§Ù‹ Ù…Ù‡Ù…Ø§Ù‹ Ø¹Ù„Ù‰ Ù†Ø¸ÙŠØ±Ù‡ Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠ Ø¨Ù†ØªÙŠØ¬Ø© 2-1.
    Ø³Ø¬Ù„ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙƒÙ„ Ù…Ù† Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ ÙˆÙŠÙˆØ³Ù Ø£Ø­Ù…Ø¯ ÙÙŠ Ø§Ù„Ø´ÙˆØ· Ø§Ù„Ø«Ø§Ù†ÙŠ.
    """
    
    print("=" * 60)
    print("ğŸ¤– Content Extractor Test")
    print("=" * 60)
    
    extractor = ContentExtractor()
    result = extractor.extract_news(test_content, "https://example.com")
    
    print(f"\nâœ… Success: {result.success}")
    print(f"ğŸ“° News Count: {result.total_extracted}")
    
    for i, news in enumerate(result.news_items, 1):
        print(f"\n--- News #{i} ---")
        print(f"ğŸ“Œ Title: {news.title}")
        print(f"ğŸ“ Category: {news.category}")
        print(f"ğŸ·ï¸ Tags: {news.tags_str}")
        print(f"ğŸ“ Content: {news.content[:100]}...")