#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ§  Smart Web Scraper
Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ„ Ø£Ù†ÙˆØ§Ø¹ ØµÙØ­Ø§Øª Ø§Ù„ÙˆÙŠØ¨

Supported Page Types:
1. Static Pages - requests + BeautifulSoup
2. Dynamic Pages - Playwright (JS-rendered)
3. Hybrid Pages - requests with Playwright fallback
4. Listing Pages - Extract links only
5. Custom Layout - Site-specific selectors
6. Protected Pages - Custom headers/cookies
7. Invalid Pages - Skip patterns
"""

import re
import time
import requests
import warnings
from enum import Enum
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass, field
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from datetime import datetime, timezone

# ØªØ¬Ø§Ù‡Ù„ ØªØ­Ø°ÙŠØ±Ø§Øª SSL
warnings.filterwarnings('ignore', message='Unverified HTTPS request')

# Playwright
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸ Playwright not installed. Dynamic pages won't work.")
    print("   Install: pip install playwright && playwright install chromium")


# ============================================
# ğŸ“Š Enums & Data Classes
# ============================================

class PageType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØµÙØ­Ø§Øª"""
    STATIC = "static"           # HTML Ø«Ø§Ø¨Øª
    DYNAMIC = "dynamic"         # JavaScript-rendered
    HYBRID = "hybrid"           # Ø¬Ø²Ø¡ Ø«Ø§Ø¨Øª + JS
    LISTING = "listing"         # ØµÙØ­Ø© Ù‚ÙˆØ§Ø¦Ù…/Ø±ÙˆØ§Ø¨Ø·
    CUSTOM = "custom"           # ØªØµÙ…ÙŠÙ… Ø®Ø§Øµ
    PROTECTED = "protected"     # Ù…Ø­Ù…ÙŠØ©
    INVALID = "invalid"         # ØºÙŠØ± ØµØ§Ù„Ø­Ø©


class ContentStatus(Enum):
    """Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
    FULL = "full"               # Ù…Ø­ØªÙˆÙ‰ ÙƒØ§Ù…Ù„
    PARTIAL = "partial"         # Ù…Ø­ØªÙˆÙ‰ Ø¬Ø²Ø¦ÙŠ
    EMPTY = "empty"             # ÙØ§Ø±Øº
    LINKS_ONLY = "links_only"   # Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·


@dataclass
class ScrapeResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø³Ø­Ø¨"""
    success: bool
    url: str
    page_type: PageType
    content_status: ContentStatus
    
    # Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    title: str = ""
    content: str = ""
    clean_text: str = ""
    
    # Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
    images: List[str] = field(default_factory=list)
    videos: List[str] = field(default_factory=list)
    
    # Ø§Ù„Ø±ÙˆØ§Ø¨Ø· (Ù„Ù„Ù€ listing pages)
    article_links: List[str] = field(default_factory=list)
    
    # Meta
    meta_description: str = ""
    published_date: Optional[str] = None
    author: Optional[str] = None
    
    # Ø§Ù„ØªÙ‚Ù†ÙŠØ©
    method_used: str = "requests"  # requests | playwright
    fallback_used: bool = False
    
    # Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    error_message: Optional[str] = None


# ============================================
# âš™ï¸ Site-Specific Configurations
# ============================================

SITE_CONFIGS: Dict[str, Dict] = {
    # Ù…Ø«Ø§Ù„: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø®Ø§ØµØ© Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¹ÙŠÙ†
    # "example.com": {
    #     "content_selector": ".article-body",
    #     "title_selector": "h1.title",
    #     "page_type": PageType.CUSTOM,
    #     "requires_browser": True,
    #     "wait_selector": ".content-loaded",
    #     "cookies": [{"name": "consent", "value": "true"}]
    # },
    
    # Ø§Ù„Ø¬Ø²ÙŠØ±Ø© - Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
    "aljazeera.net": {
        "page_type": PageType.DYNAMIC,
        "requires_browser": True,
        "content_selector": ".wysiwyg",
        "wait_for": 2000,
    },
    
    # Ø¹Ø±Ø¨ 48
    "arab48.com": {
        "page_type": PageType.DYNAMIC,
        "requires_browser": True,
        "content_selector": ".article-content",
    },
    
    # ÙˆÙƒØ§Ù„Ø© Ù…Ø¹Ø§
    "maannews.net": {
        "page_type": PageType.DYNAMIC,
        "requires_browser": True,
        "content_selector": ".article-body, .news-content",
    },
}


# ============================================
# ğŸš« Skip Patterns (Invalid Pages)
# ============================================

SKIP_URL_PATTERNS = [
    r'/login',
    r'/register',
    r'/signup',
    r'/signin',
    r'/search',
    r'/tag/',
    r'/tags/',
    r'/category/',
    r'/categories/',
    r'/author/',
    r'/page/\d+',
    r'/video/',
    r'/videos/',
    r'/gallery/',
    r'/photos/',
    r'/contact',
    r'/about',
    r'/privacy',
    r'/terms',
    r'/faq',
    r'/rss',
    r'/feed',
    r'/sitemap',
    r'/archive',
    r'\.pdf$',
    r'\.jpg$',
    r'\.png$',
    r'\.mp4$',
    r'\.mp3$',
    r'^#',
    r'^javascript:',
    r'^mailto:',
    r'^tel:',
]


# ============================================
# ğŸ“° Article URL Patterns
# ============================================

ARTICLE_URL_PATTERNS = [
    r'/\d{4}/\d{1,2}/\d{1,2}/',  # /2024/12/31/
    r'/\d{4}/\d{1,2}/',          # /2024/12/
    r'/news/\d+',                 # /news/123456
    r'/article/\d+',              # /article/123
    r'/story/\d+',                # /story/123
    r'/post/\d+',                 # /post/123
    r'[?&]id=\d+',               # ?id=123
    r'[?&]p=\d+',                # ?p=123
    r'-\d{5,}',                   # -123456
    r'/\d{6,}/?$',               # /123456 or /123456/
]


# ============================================
# ğŸ§  Smart Scraper Class
# ============================================

class SmartScraper:
    """
    ğŸ§  Smart Web Scraper
    ÙŠÙƒØªØ´Ù Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙˆÙŠØ®ØªØ§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
    """
    
    # Thresholds
    MIN_CONTENT_LENGTH = 100      # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ù…Ø­ØªÙˆÙ‰
    HYBRID_THRESHOLD = 300        # Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† hybrid
    
    # Content Selectors (Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù„Ù„Ø£Ø¹Ù…)
    CONTENT_SELECTORS = [
        'article .article-content',
        'article .post-content', 
        'article .entry-content',
        '.article-body',
        '.article-content',
        '.post-body',
        '.post-content',
        '.news-body',
        '.news-content',
        '.story-body',
        '.story-content',
        '.entry-content',
        '.single-content',
        '#article-body',
        '#post-content',
        '.wysiwyg',
        '.rich-text',
        'article',
        '[role="article"]',
        'main article',
        '.main-content article',
    ]
    
    # Title Selectors
    TITLE_SELECTORS = [
        'h1.article-title',
        'h1.post-title',
        'h1.entry-title',
        '.article-header h1',
        '.post-header h1',
        'article h1',
        'main h1',
        'h1',
    ]
    
    # Elements to Remove
    REMOVE_SELECTORS = [
        'script', 'style', 'nav', 'header', 'footer',
        'aside', 'iframe', 'form', 'noscript',
        '.ad', '.ads', '.advertisement', '.banner',
        '.social-share', '.share-buttons', '.sharing',
        '.related-posts', '.related-articles', '.recommended',
        '.comments', '.comment-section',
        '.sidebar', '.widget',
        '.newsletter', '.subscribe',
        '.popup', '.modal',
        '[class*="ad-"]', '[class*="ads-"]',
        '[id*="ad-"]', '[id*="ads-"]',
    ]
    
    def __init__(
        self,
        timeout: int = 30,
        min_content_length: int = 100,
        use_browser: bool = True
    ):
        self.timeout = timeout
        self.min_content_length = min_content_length
        self.use_browser = use_browser and PLAYWRIGHT_AVAILABLE
        
        # Session for requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # Playwright browser (lazy init)
        self._browser = None
        self._playwright = None
    
    def scrape(self, url: str) -> ScrapeResult:
        """
        Ø³Ø­Ø¨ ØµÙØ­Ø© Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ
        
        1. ÙŠÙƒØªØ´Ù Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©
        2. ÙŠØ®ØªØ§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
        3. ÙŠØ¬Ø±Ø¨ fallback Ø¥Ø°Ø§ ÙØ´Ù„
        """
        domain = self._get_domain(url)
        
        # Step 1: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©
        if self._is_invalid_url(url):
            return ScrapeResult(
                success=False,
                url=url,
                page_type=PageType.INVALID,
                content_status=ContentStatus.EMPTY,
                error_message="Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­ Ù„Ù„Ø³Ø­Ø¨"
            )
        
        # Step 2: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† site-specific config
        site_config = self._get_site_config(domain)
        
        # Step 3: ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©
        page_type = self._detect_page_type(url, site_config)
        
        # Step 4: Ø§Ù„Ø³Ø­Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        if page_type == PageType.LISTING:
            return self._scrape_listing_page(url, site_config)
        
        if page_type == PageType.DYNAMIC or site_config.get('requires_browser'):
            return self._scrape_with_browser(url, site_config, page_type)
        
        # Try requests first (Static/Hybrid)
        result = self._scrape_with_requests(url, site_config)
        
        # Fallback to browser if content is insufficient
        if result.content_status in [ContentStatus.EMPTY, ContentStatus.PARTIAL]:
            if self.use_browser:
                print(f"   âš ï¸ Content insufficient ({len(result.clean_text)} chars), trying browser...")
                browser_result = self._scrape_with_browser(url, site_config, PageType.HYBRID)
                browser_result.fallback_used = True
                return browser_result
        
        return result
    
    def scrape_multiple(self, urls: List[str], delay: float = 1.0) -> List[ScrapeResult]:
        """Ø³Ø­Ø¨ Ø¹Ø¯Ø© ØµÙØ­Ø§Øª"""
        results = []
        
        for i, url in enumerate(urls, 1):
            print(f"   [{i}/{len(urls)}] {url[:50]}...")
            result = self.scrape(url)
            results.append(result)
            
            if i < len(urls):
                time.sleep(delay)
        
        self._close_browser()
        return results
    
    # ============================================
    # ğŸ” Detection Methods
    # ============================================
    
    def _detect_page_type(self, url: str, site_config: Dict) -> PageType:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©"""
        
        # Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        if site_config.get('page_type'):
            return site_config['page_type']
        
        # ØµÙØ­Ø© Ù‚ÙˆØ§Ø¦Ù…ØŸ
        if self._is_listing_url(url):
            return PageType.LISTING
        
        # Ø§ÙØªØ±Ø§Ø¶ÙŠ: hybrid (Ù†Ø¬Ø±Ø¨ requests Ø«Ù… browser)
        return PageType.HYBRID
    
    def _is_invalid_url(self, url: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­"""
        path = urlparse(url).path.lower()
        
        for pattern in SKIP_URL_PATTERNS:
            if re.search(pattern, path) or re.search(pattern, url.lower()):
                return True
        return False
    
    def _is_listing_url(self, url: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙØ­Ø© Ù‚ÙˆØ§Ø¦Ù…"""
        parsed = urlparse(url)
        path = parsed.path.rstrip('/')
        
        # Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        if not path or path == '/':
            return True
        
        # Ù…Ø³Ø§Ø±Ø§Øª Ù‚ØµÙŠØ±Ø© Ø¨Ø¯ÙˆÙ† Ø£Ø±Ù‚Ø§Ù…
        parts = [p for p in path.split('/') if p]
        if len(parts) <= 2 and not any(re.search(r'\d{4,}', p) for p in parts):
            # ØªØ­Ù‚Ù‚ Ø¥Ø¶Ø§ÙÙŠ: Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ù†Ù…Ø· Ù…Ù‚Ø§Ù„ØŸ
            if not any(re.search(pattern, url) for pattern in ARTICLE_URL_PATTERNS):
                return True
        
        return False
    
    def _is_article_url(self, url: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ø§Ø¨Ø· Ù…Ù‚Ø§Ù„"""
        for pattern in ARTICLE_URL_PATTERNS:
            if re.search(pattern, url):
                return True
        return False
    
    def _get_site_config(self, domain: str) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        # Ø¥Ø²Ø§Ù„Ø© www
        clean_domain = domain.replace('www.', '')
        
        return SITE_CONFIGS.get(clean_domain, {})
    
    def _get_domain(self, url: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†"""
        try:
            return urlparse(url).netloc
        except:
            return ""
    
    # ============================================
    # ğŸ“¥ Scraping Methods
    # ============================================
    
    def _scrape_with_requests(self, url: str, site_config: Dict) -> ScrapeResult:
        """Ø§Ù„Ø³Ø­Ø¨ Ø¨Ù€ requests"""
        try:
            response = self.session.get(
                url,
                timeout=self.timeout,
                verify=False,
                allow_redirects=True
            )
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            
            html = response.text
            return self._parse_html(html, url, "requests", site_config)
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code in [403, 401]:
                return ScrapeResult(
                    success=False,
                    url=url,
                    page_type=PageType.PROTECTED,
                    content_status=ContentStatus.EMPTY,
                    error_message=f"ØµÙØ­Ø© Ù…Ø­Ù…ÙŠØ©: {e.response.status_code}"
                )
            raise
        except Exception as e:
            return ScrapeResult(
                success=False,
                url=url,
                page_type=PageType.STATIC,
                content_status=ContentStatus.EMPTY,
                error_message=str(e)
            )
    
    def _scrape_with_browser(self, url: str, site_config: Dict, page_type: PageType) -> ScrapeResult:
        """Ø§Ù„Ø³Ø­Ø¨ Ø¨Ù€ Playwright"""
        if not PLAYWRIGHT_AVAILABLE:
            return ScrapeResult(
                success=False,
                url=url,
                page_type=page_type,
                content_status=ContentStatus.EMPTY,
                error_message="Playwright ØºÙŠØ± Ù…ØªÙˆÙØ±"
            )
        
        try:
            # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØµÙØ­
            if not self._browser:
                self._init_browser()
            
            page = self._browser.new_page()
            
            # Ø¥Ø¶Ø§ÙØ© cookies Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø©
            if site_config.get('cookies'):
                page.context.add_cookies(site_config['cookies'])
            
            # Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù„Ù„ØµÙØ­Ø©
            page.goto(url, wait_until='domcontentloaded', timeout=self.timeout * 1000)
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ø¥Ø¶Ø§ÙÙŠ
            wait_time = site_config.get('wait_for', 2000)
            page.wait_for_timeout(wait_time)
            
            # Ø§Ù†ØªØ¸Ø§Ø± selector Ù…Ø¹ÙŠÙ†
            if site_config.get('wait_selector'):
                try:
                    page.wait_for_selector(site_config['wait_selector'], timeout=5000)
                except:
                    pass
            
            html = page.content()
            page.close()
            
            return self._parse_html(html, url, "playwright", site_config)
            
        except Exception as e:
            return ScrapeResult(
                success=False,
                url=url,
                page_type=page_type,
                content_status=ContentStatus.EMPTY,
                error_message=str(e)
            )
    
    def _scrape_listing_page(self, url: str, site_config: Dict) -> ScrapeResult:
        """Ø³Ø­Ø¨ ØµÙØ­Ø© Ù‚ÙˆØ§Ø¦Ù… - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·"""
        
        # Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©
        if site_config.get('requires_browser') and self.use_browser:
            result = self._scrape_with_browser(url, site_config, PageType.LISTING)
            html = result.content  # Ù†Ø³ØªØ®Ø¯Ù… raw HTML
        else:
            try:
                response = self.session.get(url, timeout=self.timeout, verify=False)
                response.raise_for_status()
                html = response.text
            except:
                # fallback to browser
                if self.use_browser:
                    result = self._scrape_with_browser(url, site_config, PageType.LISTING)
                    html = result.content
                else:
                    return ScrapeResult(
                        success=False,
                        url=url,
                        page_type=PageType.LISTING,
                        content_status=ContentStatus.EMPTY,
                        error_message="ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©"
                    )
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        soup = BeautifulSoup(html, 'html.parser')
        article_links = self._extract_article_links(soup, url)
        
        return ScrapeResult(
            success=len(article_links) > 0,
            url=url,
            page_type=PageType.LISTING,
            content_status=ContentStatus.LINKS_ONLY,
            article_links=article_links,
            title=self._extract_title(soup),
            method_used="browser" if site_config.get('requires_browser') else "requests"
        )
    
    # ============================================
    # ğŸ”§ Parsing Methods
    # ============================================
    
    def _parse_html(self, html: str, url: str, method: str, site_config: Dict) -> ScrapeResult:
        """ØªØ­Ù„ÙŠÙ„ HTML ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        self._clean_soup(soup)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        title = self._extract_title(soup)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_selector = site_config.get('content_selector')
        content, clean_text = self._extract_content(soup, content_selector)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_status = self._assess_content_status(clean_text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±
        images = self._extract_images(soup, url)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        videos = self._extract_videos(soup, url)
        
        # Meta
        meta_desc = self._extract_meta_description(soup)
        pub_date = self._extract_date(soup)
        author = self._extract_author(soup)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©
        page_type = PageType.DYNAMIC if method == "playwright" else PageType.STATIC
        if content_status == ContentStatus.PARTIAL:
            page_type = PageType.HYBRID
        
        return ScrapeResult(
            success=content_status != ContentStatus.EMPTY,
            url=url,
            page_type=page_type,
            content_status=content_status,
            title=title,
            content=content,
            clean_text=clean_text,
            images=images,
            videos=videos,
            meta_description=meta_desc,
            published_date=pub_date,
            author=author,
            method_used=method
        )
    
    def _clean_soup(self, soup: BeautifulSoup) -> None:
        """ØªÙ†Ø¸ÙŠÙ DOM"""
        for selector in self.REMOVE_SELECTORS:
            try:
                for element in soup.select(selector):
                    element.decompose()
            except:
                continue
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†"""
        # Ù…Ù† og:title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            title = og_title['content'].strip()
            title = re.split(r'\s*[|\-â€“â€”]\s*', title)[0].strip()
            if len(title) > 10:
                return title
        
        # Ù…Ù† selectors
        for selector in self.TITLE_SELECTORS:
            try:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    if len(text) > 10:
                        return text
            except:
                continue
        
        # Ù…Ù† title tag
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text(strip=True).split('|')[0].strip()
        
        return ""
    
    def _extract_content(self, soup: BeautifulSoup, custom_selector: str = None) -> Tuple[str, str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        content_parts = []
        
        # Custom selector Ø£ÙˆÙ„Ø§Ù‹
        selectors = [custom_selector] if custom_selector else []
        selectors.extend(self.CONTENT_SELECTORS)
        
        for selector in selectors:
            if not selector:
                continue
            try:
                elements = soup.select(selector)
                for element in elements:
                    paragraphs = element.find_all('p')
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if len(text) > 30 and not self._is_junk(text):
                            content_parts.append(text)
                    
                    if content_parts:
                        break
                
                if content_parts:
                    break
            except:
                continue
        
        # Fallback: ÙƒÙ„ Ø§Ù„ÙÙ‚Ø±Ø§Øª
        if not content_parts:
            for p in soup.find_all('p'):
                text = p.get_text(strip=True)
                if len(text) > 50 and not self._is_junk(text):
                    content_parts.append(text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
        seen = set()
        unique_parts = []
        for part in content_parts:
            if part not in seen:
                seen.add(part)
                unique_parts.append(part)
        
        raw_content = '\n\n'.join(unique_parts[:20])
        clean_text = ' '.join(unique_parts[:20])
        
        return raw_content, clean_text
    
    def _is_junk(self, text: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Øµ ØºÙŠØ± Ù…ÙÙŠØ¯"""
        junk_patterns = [
            r'Ø§Ù‚Ø±Ø£ Ø£ÙŠØ¶Ø§', r'Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø°Ø§Øª ØµÙ„Ø©', r'Ø´Ø§Ø±Ùƒ Ø§Ù„Ø®Ø¨Ø±',
            r'ØªØ§Ø¨Ø¹Ù†Ø§ Ø¹Ù„Ù‰', r'Ø§Ù†Ø¶Ù… Ø¥Ù„Ù‰', r'Ø§Ø´ØªØ±Ùƒ ÙÙŠ',
            r'Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù†Ø´Ø±', r'Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚', r'copyright',
            r'all rights', r'Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù†', r'Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯',
        ]
        text_lower = text.lower()
        return any(re.search(p, text_lower, re.I) for p in junk_patterns)
    
    def _assess_content_status(self, clean_text: str) -> ContentStatus:
        """ØªÙ‚ÙŠÙŠÙ… Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        length = len(clean_text)
        
        if length < self.MIN_CONTENT_LENGTH:
            return ContentStatus.EMPTY
        elif length < self.HYBRID_THRESHOLD:
            return ContentStatus.PARTIAL
        else:
            return ContentStatus.FULL
    
    def _extract_article_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª"""
        domain = self._get_domain(base_url)
        links = []
        seen = set()
        
        for a in soup.find_all('a', href=True):
            href = a['href']
            full_url = urljoin(base_url, href)
            
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
            if domain not in full_url:
                continue
            
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…ÙƒØ±Ø±
            if full_url in seen:
                continue
            seen.add(full_url)
            
            # ØªØ¬Ø§Ù‡Ù„ ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©
            if self._is_invalid_url(full_url):
                continue
            
            # ÙÙ‚Ø· Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            if self._is_article_url(full_url):
                links.append(full_url)
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø­Ø¯Ø«
        links.sort(key=lambda x: self._extract_id(x), reverse=True)
        
        return links
    
    def _extract_id(self, url: str) -> int:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ID Ù„Ù„ØªØ±ØªÙŠØ¨"""
        numbers = re.findall(r'\d{5,}', url)
        return int(numbers[-1]) if numbers else 0
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±"""
        images = []
        seen = set()
        
        # og:image
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            full_url = urljoin(base_url, og_image['content'])
            images.append(full_url)
            seen.add(full_url)
        
        # img tags
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                full_url = urljoin(base_url, src)
                if full_url not in seen and self._is_valid_image(full_url):
                    images.append(full_url)
                    seen.add(full_url)
        
        return images[:10]
    
    def _is_valid_image(self, url: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙˆØ±Ø© ØµØ§Ù„Ø­Ø©"""
        skip = [r'icon', r'logo', r'avatar', r'button', r'pixel', r'1x1', r'\.gif$', r'\.ico$']
        url_lower = url.lower()
        return not any(re.search(p, url_lower) for p in skip)
    
    def _extract_videos(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        videos = []
        
        for video in soup.find_all('video'):
            src = video.get('src')
            if src:
                videos.append(urljoin(base_url, src))
        
        for iframe in soup.find_all('iframe'):
            src = iframe.get('src', '')
            if any(d in src for d in ['youtube.com', 'vimeo.com', 'dailymotion']):
                videos.append(src)
        
        return videos[:5]
    
    def _extract_meta_description(self, soup: BeautifulSoup) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ"""
        for prop in ['og:description', 'description']:
            meta = soup.find('meta', attrs={'property': prop}) or soup.find('meta', attrs={'name': prop})
            if meta and meta.get('content'):
                return meta['content'].strip()
        return ""
    
    def _extract_date(self, soup: BeautifulSoup) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®"""
        for attr, value in [('property', 'article:published_time'), ('name', 'date')]:
            meta = soup.find('meta', attrs={attr: value})
            if meta and meta.get('content'):
                return meta['content']
        
        time_tag = soup.find('time')
        if time_tag:
            return time_tag.get('datetime') or time_tag.get_text(strip=True)
        
        return None
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØ§ØªØ¨"""
        meta = soup.find('meta', attrs={'name': 'author'})
        if meta and meta.get('content'):
            return meta['content']
        
        author_link = soup.find('a', rel='author')
        if author_link:
            return author_link.get_text(strip=True)
        
        return None
    
    # ============================================
    # ğŸ”§ Browser Management
    # ============================================
    
    def _init_browser(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØµÙØ­"""
        if not PLAYWRIGHT_AVAILABLE:
            return
        
        self._playwright = sync_playwright().start()
        self._browser = self._playwright.chromium.launch(headless=True)
    
    def _close_browser(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­"""
        try:
            if self._browser:
                self._browser.close()
            if self._playwright:
                self._playwright.stop()
        except:
            pass
        finally:
            self._browser = None
            self._playwright = None
    
    def __del__(self):
        """Cleanup"""
        self._close_browser()


# ============================================
# ğŸš€ Helper Functions
# ============================================

def smart_scrape(url: str, use_browser: bool = True) -> ScrapeResult:
    """Ø¯Ø§Ù„Ø© Ù…Ø®ØªØµØ±Ø© Ù„Ù„Ø³Ø­Ø¨ Ø§Ù„Ø°ÙƒÙŠ"""
    scraper = SmartScraper(use_browser=use_browser)
    result = scraper.scrape(url)
    scraper._close_browser()
    return result


def scrape_article(url: str) -> ScrapeResult:
    """Ø³Ø­Ø¨ Ù…Ù‚Ø§Ù„"""
    return smart_scrape(url)


def get_article_links(url: str) -> List[str]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ù† ØµÙØ­Ø©"""
    scraper = SmartScraper()
    result = scraper.scrape(url)
    scraper._close_browser()
    return result.article_links