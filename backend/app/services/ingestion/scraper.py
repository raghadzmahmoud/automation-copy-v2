#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ“° News Scraper Service
Ø®Ø¯Ù…Ø© Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feeds

ğŸ“ S3 Paths:
   - original/images/  â† ØµÙˆØ± Ø£ØµÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
   - original/videos/  â† ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø£ØµÙ„ÙŠØ© (Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹)
"""

import os
import time
import re
import hashlib
import feedparser
import requests
import boto3
from botocore.exceptions import ClientError
from datetime import datetime, timezone
from typing import List, Dict, Optional, Tuple

from settings import GEMINI_API_KEY, GEMINI_MODEL
from app.config.user_config import user_config
from app.utils.database import (
    get_source_id,
    get_source_last_fetched,
    update_source_last_fetched,
    get_language_id,
    get_or_create_category_id,
    save_news_batch
)
from app.services.processing.classifier import classify_with_gemini


class NewsScraper:
    """
    News Scraper - Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feeds
    Ù…Ø¹ Ø¯Ø¹Ù… Ø±ÙØ¹ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¹Ù„Ù‰ S3
    """
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø³Ø­Ø¨"""
        self.timeout = user_config.scraping_timeout_seconds
        self.max_news_per_source = user_config.max_news_per_source
        
        # ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.processed_titles = set()
        
        # ØªÙ‡ÙŠØ¦Ø© S3 Client Ù„Ù„ØµÙˆØ± Ø§Ù„Ø£ØµÙ„ÙŠØ©
        try:
            self.s3_client = boto3.client('s3')
            self.bucket_name = os.getenv('S3_BUCKET_NAME', 'media-automation-bucket')
            
            # âœ… Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
            self.s3_original_images_folder = os.getenv('S3_ORIGINAL_IMAGES_FOLDER', 'original/images/')
            self.s3_original_videos_folder = os.getenv('S3_ORIGINAL_VIDEOS_FOLDER', 'original/videos/')
            
            self.upload_to_s3 = True
            print(f"âœ… S3 client initialized for original media")
            print(f"   ğŸ“ Images folder: {self.s3_original_images_folder}")
            print(f"   ğŸ“ Videos folder: {self.s3_original_videos_folder}")
        except Exception as e:
            print(f"âš ï¸  S3 client not available: {e}")
            self.upload_to_s3 = False
    
    def scrape_rss(self, url: str, source_id: int, language_id: int) -> List[Dict]:
        """
        Ø³Ø­Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feed
        
        Args:
            url: Ø±Ø§Ø¨Ø· RSS
            source_id: ID Ø§Ù„Ù…ØµØ¯Ø±
            language_id: ID Ø§Ù„Ù„ØºØ©
        
        Returns:
            List[Dict]: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        """
        news_list = []
        
        try:
            # Ø¬Ù„Ø¨ RSS feed
            feed = self._fetch_rss(url)
            
            if not feed.entries:
                print(f"   âš ï¸  No entries found")
                return []
            
            print(f"   ğŸ“Š Found {len(feed.entries)} entries")
            
            # last_fetched Ù„Ù„ÙÙ„ØªØ±Ø©
            last_fetched = get_source_last_fetched(source_id)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ entry
            count = 0
            for idx, entry in enumerate(feed.entries, 1):
                # Ø­Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
                if self.max_news_per_source and count >= self.max_news_per_source:
                    break
                
                try:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    news_item = self._process_entry(
                        entry, 
                        source_id, 
                        language_id,
                        last_fetched
                    )
                    
                    if news_item:
                        news_list.append(news_item)
                        count += 1
                        
                        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø¯Ù…
                        print(f"   [{count:3d}] âœ“ {news_item['title'][:40]}...")
                        
                        # Ø±Ø§Ø­Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
                        time.sleep(3)
                
                except Exception as e:
                    print(f"   [{idx:3d}] âŒ Error: {str(e)[:50]}")
                    continue
            
            # ØªØ­Ø¯ÙŠØ« last_fetched
            if news_list:
                update_source_last_fetched(source_id)
            
            return news_list
            
        except Exception as e:
            print(f"   âŒ Scraping error: {e}")
            return []
    
    def _fetch_rss(self, url: str):
        """Ø¬Ù„Ø¨ RSS feed"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/rss+xml, application/xml, */*',
            }
            
            response = requests.get(
                url,
                headers=headers,
                timeout=self.timeout,
                verify=False,
                allow_redirects=True
            )
            
            response.raise_for_status()
            return feedparser.parse(response.content)
            
        except Exception as e:
            print(f"   âŒ Fetch error: {e}")
            return feedparser.parse("")
    
    def _process_entry(
        self, 
        entry, 
        source_id: int, 
        language_id: int,
        last_fetched: Optional[datetime]
    ) -> Optional[Dict]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© entry ÙˆØ§Ø­Ø¯"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        title = self._clean_html(entry.get('title', ''))
        
        # ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø±
        if self._is_duplicate(title):
            return None
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®
        pub_date = self._extract_date(entry)
        if not pub_date:
            pub_date = datetime.now(timezone.utc)
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…
        if last_fetched and pub_date.replace(tzinfo=timezone.utc) <= last_fetched.replace(tzinfo=timezone.utc):
            return None
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content = self._extract_content(entry)
        content_text = self._clean_html(content)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©
        if len(title) < 10 and len(content_text) < 20:
            return None
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ
        original_image_url = self._extract_image(entry)
        original_video_url = self._extract_video(entry)
        
        # âœ… Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¹Ù„Ù‰ S3: original/images/
        content_img = ""
        if original_image_url and self.upload_to_s3:
            s3_image_url = self._upload_original_image_to_s3(
                image_url=original_image_url,
                source_id=source_id
            )
            content_img = s3_image_url if s3_image_url else original_image_url
        else:
            content_img = original_image_url
        
        # Ø§Ù„ÙÙŠØ¯ÙŠÙˆ - Ø­Ø§Ù„ÙŠØ§Ù‹ Ù†Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ
        content_video = original_video_url
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø§Ù„Ù€ AI
        category_name, tags_str, tags_list, ai_success = classify_with_gemini(
            title, 
            content_text,
            max_retries=3
        )
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ category_id
        category_id = get_or_create_category_id(category_name)
        
        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.processed_titles.add(title.lower())
        
        return {
            'title': title,
            'content_text': content_text,
            'content_img': content_img,
            'content_video': content_video,
            'tags': tags_str,
            'source_id': source_id,
            'language_id': language_id,
            'category_id': category_id,
            'published_at': pub_date,
            'collected_at': datetime.now(timezone.utc)
        }
    
    def _upload_original_image_to_s3(
        self, 
        image_url: str, 
        source_id: int
    ) -> Optional[str]:
        """
        âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ±ÙØ¹Ù‡Ø§ Ø¹Ù„Ù‰ S3
        
        Args:
            image_url: Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
            source_id: ID Ø§Ù„Ù…ØµØ¯Ø±
        
        Returns:
            str: Ø±Ø§Ø¨Ø· S3 Ø£Ùˆ None
        """
        if not image_url or not self.upload_to_s3:
            return None
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            }
            
            response = requests.get(
                image_url,
                headers=headers,
                timeout=15,
                verify=False
            )
            
            if response.status_code != 200:
                print(f"      âš ï¸  Image download failed: {response.status_code}")
                return None
            
            image_bytes = response.content
            
            if len(image_bytes) < 1000:  # Ø£Ù‚Ù„ Ù…Ù† 1KB
                print(f"      âš ï¸  Image too small, skipping")
                return None
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØ±Ø©
            content_type = response.headers.get('Content-Type', 'image/jpeg')
            if 'png' in content_type.lower() or image_url.lower().endswith('.png'):
                extension = 'png'
                content_type = 'image/png'
            elif 'gif' in content_type.lower() or image_url.lower().endswith('.gif'):
                extension = 'gif'
                content_type = 'image/gif'
            elif 'webp' in content_type.lower() or image_url.lower().endswith('.webp'):
                extension = 'webp'
                content_type = 'image/webp'
            else:
                extension = 'jpg'
                content_type = 'image/jpeg'
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… ÙØ±ÙŠØ¯ Ù„Ù„Ù…Ù„Ù
            url_hash = hashlib.md5(image_url.encode()).hexdigest()[:12]
            timestamp = int(time.time())
            file_name = f"source_{source_id}_{timestamp}_{url_hash}.{extension}"
            
            # âœ… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­: original/images/
            s3_key = f"{self.s3_original_images_folder}{file_name}"
            
            # Ø±ÙØ¹ Ø¹Ù„Ù‰ S3
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=s3_key,
                Body=image_bytes,
                ContentType=content_type
            )
            
            s3_url = f"https://{self.bucket_name}.s3.amazonaws.com/{s3_key}"
            print(f"      ğŸ“¤ Image uploaded to S3: {s3_key}")
            
            return s3_url
            
        except requests.exceptions.Timeout:
            print(f"      âš ï¸  Image download timeout")
            return None
        except ClientError as e:
            print(f"      âš ï¸  S3 upload error: {e}")
            return None
        except Exception as e:
            print(f"      âš ï¸  Image upload error: {str(e)[:50]}")
            return None
    
    def _extract_date(self, entry) -> Optional[datetime]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®"""
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            try:
                return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            except:
                pass
        
        if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            try:
                return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
            except:
                pass
        
        if hasattr(entry, 'published') and entry.published:
            date_str = entry.published.strip()
            
            match = re.match(r'(\d{2})/(\d{2})/(\d{4})\s*-?\s*(\d{2}):(\d{2})', date_str)
            if match:
                day, month, year, hour, minute = match.groups()
                try:
                    return datetime(
                        int(year), int(month), int(day),
                        int(hour), int(minute),
                        tzinfo=timezone.utc
                    )
                except:
                    pass
        
        return None
    
    def _extract_content(self, entry) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        if hasattr(entry, 'content') and entry.content:
            return entry.content[0].value
        elif hasattr(entry, 'summary'):
            return entry.summary
        elif hasattr(entry, 'description'):
            return entry.description
        return ""
    
    def _extract_image(self, entry) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©"""
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if 'url' in media:
                    url = media['url']
                    if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                        return url
        
        content = self._extract_content(entry)
        img_pattern = r'<img[^>]+src=["\']([^"\']+)["\']'
        matches = re.findall(img_pattern, content)
        for match in matches:
            if any(ext in match.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                return match
        
        return ""
    
    def _extract_video(self, entry) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if 'url' in media and media.get('type', '').startswith('video'):
                    return media['url']
        
        content = self._extract_content(entry)
        video_pattern = r'https?://[^\s<>"]+\.(?:mp4|webm|ogg|m4v)'
        matches = re.findall(video_pattern, content, re.IGNORECASE)
        if matches:
            return matches[0]
        
        return ""
    
    def _clean_html(self, text: str) -> str:
        """Ø¥Ø²Ø§Ù„Ø© HTML tags"""
        if not text:
            return ""
        
        text = re.sub(r'<.*?>', '', text)
        
        import html
        text = html.unescape(text)
        
        text = ' '.join(text.split())
        
        return text.strip()
    
    def _is_duplicate(self, title: str) -> bool:
        """ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        if not title:
            return False
        normalized = title.lower().strip()
        return normalized in self.processed_titles
    
    def save_news_items(self, news_list: List[Dict]) -> int:
        """
        Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Returns:
            int: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
        """
        return save_news_batch(news_list)