#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ“° News Scraper Service
Ø®Ø¯Ù…Ø© Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feeds
"""

import time
import re
import feedparser
import requests
from datetime import datetime, timezone
from typing import List, Dict, Optional, Tuple

from settings import GEMINI_API_KEY, GEMINI_MODEL
from app.config.user_config import user_config
from app.utils.database import (
    get_source_id,
    get_source_last_fetched,
    update_source_last_fetched,
    get_language_id,
    get_or_create_category_id,
    save_news_batch
)
from app.services.processing.classifier import classify_with_gemini


class NewsScraper:
    """
    News Scraper - Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feeds
    """
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø³Ø­Ø¨"""
        self.timeout = user_config.scraping_timeout_seconds
        self.max_news_per_source = user_config.max_news_per_source
        
        # ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.processed_titles = set()
    
    def scrape_rss(self, url: str, source_id: int, language_id: int) -> List[Dict]:
        """
        Ø³Ø­Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feed
        
        Args:
            url: Ø±Ø§Ø¨Ø· RSS
            source_id: ID Ø§Ù„Ù…ØµØ¯Ø±
            language_id: ID Ø§Ù„Ù„ØºØ©
        
        Returns:
            List[Dict]: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        """
        news_list = []
        
        try:
            # Ø¬Ù„Ø¨ RSS feed
            feed = self._fetch_rss(url)
            
            if not feed.entries:
                print(f"   âš ï¸  No entries found")
                return []
            
            print(f"   ğŸ“Š Found {len(feed.entries)} entries")
            
            # last_fetched Ù„Ù„ÙÙ„ØªØ±Ø©
            last_fetched = get_source_last_fetched(source_id)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ entry
            count = 0
            for idx, entry in enumerate(feed.entries, 1):
                # Ø­Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
                if self.max_news_per_source and count >= self.max_news_per_source:
                    break
                
                try:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    news_item = self._process_entry(
                        entry, 
                        source_id, 
                        language_id,
                        last_fetched
                    )
                    
                    if news_item:
                        news_list.append(news_item)
                        count += 1
                        
                        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø¯Ù…
                        print(f"   [{count:3d}] âœ“ {news_item['title'][:40]}...")
                        
                        # Ø±Ø§Ø­Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
                        time.sleep(3)
                
                except Exception as e:
                    print(f"   [{idx:3d}] âŒ Error: {str(e)[:50]}")
                    continue
            
            # ØªØ­Ø¯ÙŠØ« last_fetched
            if news_list:
                update_source_last_fetched(source_id)
            
            return news_list
            
        except Exception as e:
            print(f"   âŒ Scraping error: {e}")
            return []
    
    def _fetch_rss(self, url: str):
        """Ø¬Ù„Ø¨ RSS feed"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/rss+xml, application/xml, */*',
            }
            
            response = requests.get(
                url,
                headers=headers,
                timeout=self.timeout,
                verify=False,
                allow_redirects=True
            )
            
            response.raise_for_status()
            return feedparser.parse(response.content)
            
        except Exception as e:
            print(f"   âŒ Fetch error: {e}")
            return feedparser.parse("")
    
    def _process_entry(
        self, 
        entry, 
        source_id: int, 
        language_id: int,
        last_fetched: Optional[datetime]
    ) -> Optional[Dict]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© entry ÙˆØ§Ø­Ø¯"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        title = self._clean_html(entry.get('title', ''))
        
        # ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø±
        if self._is_duplicate(title):
            return None
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®
        pub_date = self._extract_date(entry)
        if not pub_date:
            pub_date = datetime.now(timezone.utc)
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…
        if last_fetched and pub_date.replace(tzinfo=timezone.utc) <= last_fetched.replace(tzinfo=timezone.utc):
            return None
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content = self._extract_content(entry)
        content_text = self._clean_html(content)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©
        if len(title) < 10 and len(content_text) < 20:
            return None
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ
        content_img = self._extract_image(entry)
        content_video = self._extract_video(entry)
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø§Ù„Ù€ AI
        category_name, tags_str, tags_list, ai_success = classify_with_gemini(
            title, 
            content_text,
            max_retries=3
        )
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ category_id
        category_id = get_or_create_category_id(category_name)
        
        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.processed_titles.add(title.lower())
        
        return {
            'title': title,
            'content_text': content_text,
            'content_img': content_img,
            'content_video': content_video,
            'tags': tags_str,
            'source_id': source_id,
            'language_id': language_id,
            'category_id': category_id,
            'published_at': pub_date,
            'collected_at': datetime.now(timezone.utc)
        }
    
    def _extract_date(self, entry) -> Optional[datetime]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®"""
        # Ù…Ø­Ø§ÙˆÙ„Ø© published
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            try:
                return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            except:
                pass
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© updated
        if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            try:
                return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
            except:
                pass
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© custom parsing
        if hasattr(entry, 'published') and entry.published:
            date_str = entry.published.strip()
            
            # DD/MM/YYYY - HH:MM
            match = re.match(r'(\d{2})/(\d{2})/(\d{4})\s*-?\s*(\d{2}):(\d{2})', date_str)
            if match:
                day, month, year, hour, minute = match.groups()
                try:
                    return datetime(
                        int(year), int(month), int(day),
                        int(hour), int(minute),
                        tzinfo=timezone.utc
                    )
                except:
                    pass
        
        return None
    
    def _extract_content(self, entry) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        if hasattr(entry, 'content') and entry.content:
            return entry.content[0].value
        elif hasattr(entry, 'summary'):
            return entry.summary
        elif hasattr(entry, 'description'):
            return entry.description
        return ""
    
    def _extract_image(self, entry) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©"""
        # Ù…Ù† media_content
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if 'url' in media:
                    url = media['url']
                    if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                        return url
        
        # Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content = self._extract_content(entry)
        img_pattern = r'<img[^>]+src=["\']([^"\']+)["\']'
        matches = re.findall(img_pattern, content)
        for match in matches:
            if any(ext in match.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                return match
        
        return ""
    
    def _extract_video(self, entry) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        # Ù…Ù† media_content
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if 'url' in media and media.get('type', '').startswith('video'):
                    return media['url']
        
        # Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content = self._extract_content(entry)
        video_pattern = r'https?://[^\s<>"]+\.(?:mp4|webm|ogg|m4v)'
        matches = re.findall(video_pattern, content, re.IGNORECASE)
        if matches:
            return matches[0]
        
        return ""
    
    def _clean_html(self, text: str) -> str:
        """Ø¥Ø²Ø§Ù„Ø© HTML tags"""
        if not text:
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© HTML tags
        text = re.sub(r'<.*?>', '', text)
        
        # ÙÙƒ ØªØ±Ù…ÙŠØ² HTML entities
        import html
        text = html.unescape(text)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
        text = ' '.join(text.split())
        
        return text.strip()
    
    def _is_duplicate(self, title: str) -> bool:
        """ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        if not title:
            return False
        normalized = title.lower().strip()
        return normalized in self.processed_titles
    
    def save_news_items(self, news_list: List[Dict]) -> int:
        """
        Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Returns:
            int: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
        """
        return save_news_batch(news_list)