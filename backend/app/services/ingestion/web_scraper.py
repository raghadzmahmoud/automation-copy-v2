#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸŒ Web Scraper
Ø³Ø­Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† ØµÙØ­Ø§Øª Ø§Ù„ÙˆÙŠØ¨
"""

import re
import requests
from typing import Optional, Dict, List
from dataclasses import dataclass, field
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from datetime import datetime, timezone


@dataclass
class ScrapedContent:
    """Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³Ø­ÙˆØ¨ Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
    url: str
    domain: str
    
    # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    title: str = ""
    raw_text: str = ""           # Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù… (ÙƒØ§Ù…Ù„)
    clean_text: str = ""         # Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù
    
    # Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
    images: List[str] = field(default_factory=list)
    videos: List[str] = field(default_factory=list)
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    meta_description: str = ""
    meta_keywords: str = ""
    published_date: Optional[str] = None
    author: Optional[str] = None
    
    # Ø­Ø§Ù„Ø© Ø§Ù„Ø³Ø­Ø¨
    success: bool = False
    error_message: Optional[str] = None
    scraped_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


class WebScraper:
    """
    ğŸŒ Web Scraper
    Ø³Ø­Ø¨ ÙˆØªÙ†Ø¸ÙŠÙ Ù…Ø­ØªÙˆÙ‰ ØµÙØ­Ø§Øª Ø§Ù„ÙˆÙŠØ¨
    """
    
    # Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø¥Ø²Ø§Ù„ØªÙ‡Ø§
    REMOVE_TAGS = [
        'script', 'style', 'nav', 'header', 'footer', 
        'aside', 'iframe', 'noscript', 'form', 'button',
        'input', 'select', 'textarea', 'label',
        'advertisement', 'ads', 'social-share', 'comments'
    ]
    
    # Classes/IDs Ø§Ù„ØªÙŠ ØªØ¯Ù„ Ø¹Ù„Ù‰ Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø£Ùˆ Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ù…Ù‡Ù…
    REMOVE_PATTERNS = [
        r'ad[-_]?', r'ads[-_]?', r'advert', r'banner',
        r'sidebar', r'widget', r'popup', r'modal',
        r'cookie', r'newsletter', r'subscribe',
        r'social[-_]?share', r'share[-_]?button',
        r'comment', r'related[-_]?post', r'recommended'
    ]
    
    # User Agents Ù„Ù„ØªØ¨Ø¯ÙŠÙ„
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    ]
    
    def __init__(self, timeout: int = 30):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù€ Scraper
        
        Args:
            timeout: Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
        """
        self.timeout = timeout
        self.session = requests.Session()
        self._ua_index = 0
    
    def scrape(self, url: str) -> ScrapedContent:
        """
        Ø³Ø­Ø¨ Ù…Ø­ØªÙˆÙ‰ ØµÙØ­Ø© ÙˆÙŠØ¨
        
        Args:
            url: Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø©
        
        Returns:
            ScrapedContent: Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³Ø­ÙˆØ¨
        """
        domain = self._extract_domain(url)
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©
            html = self._fetch_page(url)
            
            if not html:
                return ScrapedContent(
                    url=url,
                    domain=domain,
                    success=False,
                    error_message="ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©"
                )
            
            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            title = self._extract_title(soup)
            raw_text = self._extract_raw_text(soup)
            clean_text = self._clean_text(raw_text)
            images = self._extract_images(soup, url)
            videos = self._extract_videos(soup, url)
            meta_desc = self._extract_meta_description(soup)
            meta_keywords = self._extract_meta_keywords(soup)
            pub_date = self._extract_published_date(soup)
            author = self._extract_author(soup)
            
            return ScrapedContent(
                url=url,
                domain=domain,
                title=title,
                raw_text=raw_text,
                clean_text=clean_text,
                images=images,
                videos=videos,
                meta_description=meta_desc,
                meta_keywords=meta_keywords,
                published_date=pub_date,
                author=author,
                success=True
            )
            
        except Exception as e:
            return ScrapedContent(
                url=url,
                domain=domain,
                success=False,
                error_message=str(e)
            )
    
    def _fetch_page(self, url: str) -> Optional[str]:
        """Ø¬Ù„Ø¨ HTML Ø§Ù„ØµÙØ­Ø©"""
        try:
            headers = {
                'User-Agent': self._get_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ar,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            response = self.session.get(
                url,
                headers=headers,
                timeout=self.timeout,
                verify=False,
                allow_redirects=True
            )
            
            response.raise_for_status()
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ØµØ­ÙŠØ­
            response.encoding = response.apparent_encoding or 'utf-8'
            
            return response.text
            
        except Exception as e:
            print(f"âŒ Fetch error: {e}")
            return None
    
    def _get_user_agent(self) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ User Agent (ØªØ¨Ø¯ÙŠÙ„ Ø¯ÙˆØ±ÙŠ)"""
        ua = self.USER_AGENTS[self._ua_index % len(self.USER_AGENTS)]
        self._ua_index += 1
        return ua
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©"""
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ù† og:title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ù† title tag
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ù† h1
        h1 = soup.find('h1')
        if h1:
            return h1.get_text().strip()
        
        return ""
    
    def _extract_raw_text(self, soup: BeautifulSoup) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù… Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        # Ù†Ø³Ø®Ø© Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù„ÙŠÙ‡Ø§
        soup_copy = BeautifulSoup(str(soup), 'html.parser')
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        for tag in self.REMOVE_TAGS:
            for element in soup_copy.find_all(tag):
                element.decompose()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ class/id
        for pattern in self.REMOVE_PATTERNS:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ class
            for element in soup_copy.find_all(class_=re.compile(pattern, re.I)):
                element.decompose()
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ id
            for element in soup_copy.find_all(id=re.compile(pattern, re.I)):
                element.decompose()
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        main_content = self._find_main_content(soup_copy)
        
        if main_content:
            text = main_content.get_text(separator='\n', strip=True)
        else:
            # fallback: ÙƒÙ„ Ø§Ù„Ù†Øµ
            text = soup_copy.get_text(separator='\n', strip=True)
        
        return text
    
    def _find_main_content(self, soup: BeautifulSoup) -> Optional[BeautifulSoup]:
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† article
        article = soup.find('article')
        if article:
            return article
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† main
        main = soup.find('main')
        if main:
            return main
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† div Ø¨Ù€ class Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰
        content_classes = [
            'content', 'article', 'post', 'entry', 'story',
            'news-content', 'article-content', 'post-content',
            'entry-content', 'story-content', 'main-content'
        ]
        
        for class_name in content_classes:
            content_div = soup.find('div', class_=re.compile(class_name, re.I))
            if content_div:
                return content_div
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† div Ø¨Ù€ id Ø´Ø§Ø¦Ø¹Ø©
        content_ids = ['content', 'article', 'main', 'post', 'story']
        for id_name in content_ids:
            content_div = soup.find('div', id=re.compile(id_name, re.I))
            if content_div:
                return content_div
        
        return None
    
    def _clean_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ"""
        if not text:
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'[ \t]+', ' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© ÙˆÙ†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø³Ø·Ø±
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (ØºØ§Ù„Ø¨Ø§Ù‹ Ø²Ø¨Ø§Ù„Ø©)
        lines = text.split('\n')
        filtered_lines = []
        for line in lines:
            # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© Ù„Ù„ÙØµÙ„ØŒ Ø£Ùˆ Ø§Ù„Ø£Ø³Ø·Ø± Ø¨Ø·ÙˆÙ„ Ù…Ø¹Ù‚ÙˆÙ„
            if not line or len(line) > 20:
                filtered_lines.append(line)
        text = '\n'.join(filtered_lines)
        
        return text.strip()
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ±"""
        images = []
        seen = set()
        
        # Ù…Ù† img tags
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                full_url = urljoin(base_url, src)
                if full_url not in seen and self._is_valid_image(full_url):
                    images.append(full_url)
                    seen.add(full_url)
        
        # Ù…Ù† og:image
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            full_url = urljoin(base_url, og_image['content'])
            if full_url not in seen:
                images.insert(0, full_url)  # Ø§Ù„Ø£Ù‡Ù… ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        
        return images[:10]  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 10 ØµÙˆØ±
    
    def _extract_videos(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        videos = []
        seen = set()
        
        # Ù…Ù† video tags
        for video in soup.find_all('video'):
            src = video.get('src')
            if src:
                full_url = urljoin(base_url, src)
                if full_url not in seen:
                    videos.append(full_url)
                    seen.add(full_url)
            
            # Ù…Ù† source Ø¯Ø§Ø®Ù„ video
            for source in video.find_all('source'):
                src = source.get('src')
                if src:
                    full_url = urljoin(base_url, src)
                    if full_url not in seen:
                        videos.append(full_url)
                        seen.add(full_url)
        
        # Ù…Ù† iframes (YouTube, Vimeo, etc.)
        for iframe in soup.find_all('iframe'):
            src = iframe.get('src', '')
            if any(domain in src for domain in ['youtube.com', 'vimeo.com', 'dailymotion.com']):
                if src not in seen:
                    videos.append(src)
                    seen.add(src)
        
        return videos[:5]  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª
    
    def _is_valid_image(self, url: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø©"""
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØµÙˆØ± Ø§Ù„ØµØºÙŠØ±Ø© ÙˆØ§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª
        skip_patterns = [
            r'icon', r'logo', r'avatar', r'button',
            r'pixel', r'tracking', r'1x1', r'spacer',
            r'\.gif$', r'\.ico$'
        ]
        
        url_lower = url.lower()
        for pattern in skip_patterns:
            if re.search(pattern, url_lower):
                return False
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„ØµÙˆØ±Ø©
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp']
        return any(ext in url_lower for ext in valid_extensions)
    
    def _extract_meta_description(self, soup: BeautifulSoup) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØµÙ Ø§Ù„ØµÙØ­Ø©"""
        # og:description
        og_desc = soup.find('meta', property='og:description')
        if og_desc and og_desc.get('content'):
            return og_desc['content'].strip()
        
        # meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'].strip()
        
        return ""
    
    def _extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords and meta_keywords.get('content'):
            return meta_keywords['content'].strip()
        return ""
    
    def _extract_published_date(self, soup: BeautifulSoup) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±"""
        # Ù…Ù† meta tags
        date_metas = [
            ('property', 'article:published_time'),
            ('property', 'og:published_time'),
            ('name', 'date'),
            ('name', 'pubdate'),
            ('name', 'publish_date'),
        ]
        
        for attr, value in date_metas:
            meta = soup.find('meta', attrs={attr: value})
            if meta and meta.get('content'):
                return meta['content']
        
        # Ù…Ù† time tag
        time_tag = soup.find('time')
        if time_tag:
            return time_tag.get('datetime') or time_tag.get_text()
        
        return None
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„ÙƒØ§ØªØ¨"""
        # Ù…Ù† meta tags
        author_metas = [
            ('property', 'article:author'),
            ('name', 'author'),
            ('name', 'article:author'),
        ]
        
        for attr, value in author_metas:
            meta = soup.find('meta', attrs={attr: value})
            if meta and meta.get('content'):
                return meta['content']
        
        # Ù…Ù† rel="author"
        author_link = soup.find('a', rel='author')
        if author_link:
            return author_link.get_text().strip()
        
        return None
    
    def _extract_domain(self, url: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†"""
        try:
            parsed = urlparse(url)
            return parsed.netloc
        except:
            return ""


# ============================================
# ğŸ§ª Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
# ============================================

def scrape_url(url: str, timeout: int = 30) -> ScrapedContent:
    """
    Ø¯Ø§Ù„Ø© Ù…Ø®ØªØµØ±Ø© Ù„Ø³Ø­Ø¨ Ù…Ø­ØªÙˆÙ‰ ØµÙØ­Ø©
    
    Usage:
        from web_scraper import scrape_url
        
        content = scrape_url("https://example.com/news/article")
        print(content.clean_text)
    """
    scraper = WebScraper(timeout=timeout)
    return scraper.scrape(url)


# ============================================
# ğŸ§ª Test
# ============================================

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø±
    test_url = "https://www.aljazeera.net"
    
    print("=" * 60)
    print("ğŸŒ Web Scraper Test")
    print("=" * 60)
    print(f"\nğŸ“ URL: {test_url}")
    
    scraper = WebScraper()
    result = scraper.scrape(test_url)
    
    print(f"\nâœ… Success: {result.success}")
    print(f"ğŸ“° Title: {result.title}")
    print(f"ğŸ“ Text Length: {len(result.clean_text)} chars")
    print(f"ğŸ–¼ï¸ Images: {len(result.images)}")
    print(f"ğŸ¬ Videos: {len(result.videos)}")
    
    if result.clean_text:
        print(f"\nğŸ“„ First 500 chars:")
        print("-" * 40)
        print(result.clean_text[:500])