"""
Background Worker - Runs all cron jobs on schedule
"""
import schedule
import time
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def run_scraper():
    logger.info("ğŸ”„ Starting scraper job...")
    try:
        from cron.scraper_job import scrape_news
        scrape_news()
        logger.info("âœ… Scraper completed")
    except Exception as e:
        logger.error(f"âŒ Scraper error: {e}")

def run_clustering():
    logger.info("ğŸ”„ Starting clustering job...")
    try:
        from cron.clustering_job import cluster_news
        cluster_news()
        logger.info("âœ… Clustering completed")
    except Exception as e:
        logger.error(f"âŒ Clustering error: {e}")

def run_reports():
    logger.info("ğŸ”„ Starting reports job...")
    try:
        from cron.reports_job import generate_reports
        generate_reports()
        logger.info("âœ… Reports completed")
    except Exception as e:
        logger.error(f"âŒ Reports error: {e}")

def main():
    logger.info("=" * 60)
    logger.info("ğŸš€ Background Worker Started")
    logger.info(f"â° Time: {datetime.now()}")
    logger.info("=" * 60)
    
    # Schedule jobs
    schedule.every(10).minutes.do(run_scraper)
    schedule.every(1).hours.do(run_clustering)
    schedule.every(1).hours.do(run_reports)
    
    logger.info("ğŸ“… Scheduled Jobs:")
    logger.info("   â€¢ Scraper: every 10 minutes")
    logger.info("   â€¢ Clustering: every 1 hour")
    logger.info("   â€¢ Reports: every 1 hour")
    logger.info("=" * 60)
    
    # Run scraper immediately on startup
    logger.info("â–¶ï¸ Running initial scraper...")
    run_scraper()
    
    # Keep running
    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == "__main__":
    main()