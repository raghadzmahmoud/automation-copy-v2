#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸš€ Pipeline Queue Workers - Real-Time Event-Driven Processing
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Architecture: Hybrid

Queue-based (Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù):
  clustering â†’ report_generation â†’ image_generation
  ÙƒÙ„ Ø®Ø¨Ø± Ø¬Ø¯ÙŠØ¯ ÙŠÙ…Ø± Ø¨Ø§Ù„Ù…Ø±Ø§Ø­Ù„ ÙÙˆØ±Ø§Ù‹ Ø¨Ø¹Ø¯ Ø§Ù„Ù€ scraping

Cron-based (worker.py):
  scraping + broadcast_generation
  ÙŠØ´ØªØºÙ„ Ø­Ø³Ø¨ Ø¬Ø¯ÙˆÙ„ scheduled_tasks

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Usage:
  # ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù€ workers (Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§)
  python pipeline_queue_workers.py

  # ØªØ´ØºÙŠÙ„ worker ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·
  python pipeline_queue_workers.py --stage clustering
  python pipeline_queue_workers.py --stage report_generation
  python pipeline_queue_workers.py --stage image_generation

  # ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù€ workers ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© (Ù„Ù„ØªØ·ÙˆÙŠØ±)
  python pipeline_queue_workers.py --all-in-one
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import time
import signal
import logging
import socket
import threading
import traceback
import argparse
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, Callable

import psycopg2

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Path Setup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from settings import DB_CONFIG

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WORKER_ID       = f"{socket.gethostname()}-{os.getpid()}"
POLL_INTERVAL   = int(os.getenv('QUEUE_POLL_INTERVAL', 2))   # Ø«Ø§Ù†ÙŠØªÙŠÙ†
LOCK_TIMEOUT    = int(os.getenv('QUEUE_LOCK_TIMEOUT', 30))   # 30 Ø¯Ù‚ÙŠÙ‚Ø©
MAX_ATTEMPTS    = int(os.getenv('QUEUE_MAX_ATTEMPTS', 3))

# Retry backoff Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚
RETRY_BACKOFF = {1: 1, 2: 5, 3: 15}

# Pipeline order: ÙƒÙ„ stage ØªØ¹Ø±Ù Ø§Ù„Ù„ÙŠ Ø¨Ø¹Ø¯Ù‡Ø§
NEXT_STAGE: Dict[str, Optional[str]] = {
    'clustering':       'report_generation',
    'report_generation':'image_generation',
    'image_generation': None,   # Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù€ pipeline
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format=f'%(asctime)s - [%(name)s] - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(
            os.path.join(log_dir, f'pipeline_queue_{WORKER_ID}.log'),
            encoding='utf-8'
        ),
    ]
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Database Helpers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_conn():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¬Ø¯ÙŠØ¯ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    try:
        return psycopg2.connect(**DB_CONFIG)
    except Exception as e:
        logging.getLogger('db').error(f"âŒ DB connection failed: {e}")
        return None


def enqueue(news_id: int, stage: str, conn=None) -> bool:
    """
    Ø¥Ø¶Ø§ÙØ© Ø®Ø¨Ø± Ù„Ù…Ø±Ø­Ù„Ø© Ù…Ø¹ÙŠÙ†Ø© ÙÙŠ Ø§Ù„Ù€ queue.
    ÙŠØ³ØªØ®Ø¯Ù… ON CONFLICT DO NOTHING Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±.

    Args:
        news_id: ID Ø§Ù„Ø®Ø¨Ø±
        stage:   Ø§Ù„Ù…Ø±Ø­Ù„Ø© (clustering / report_generation / image_generation)
        conn:    Ø§ØªØµØ§Ù„ Ù…ÙˆØ¬ÙˆØ¯ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

    Returns:
        True Ø¥Ø°Ø§ Ø£ÙØ¶ÙŠÙØŒ False Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ Ø£Ùˆ ÙØ´Ù„
    """
    own_conn = conn is None
    if own_conn:
        conn = get_conn()
    if not conn:
        return False

    try:
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO news_pipeline_queue (news_id, stage, status, next_run_at)
            VALUES (%s, %s, 'pending', NOW())
            ON CONFLICT DO NOTHING
        """, (news_id, stage))
        inserted = cur.rowcount > 0
        if own_conn:
            conn.commit()
        cur.close()
        return inserted
    except Exception as e:
        logging.getLogger('queue').error(f"âŒ enqueue failed (news={news_id}, stage={stage}): {e}")
        if own_conn and conn:
            conn.rollback()
        return False
    finally:
        if own_conn and conn:
            conn.close()


def enqueue_batch(news_ids: list, stage: str = 'clustering') -> int:
    """
    Ø¥Ø¶Ø§ÙØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø®Ø¨Ø§Ø± Ù„Ù„Ù€ queue Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©.

    Args:
        news_ids: Ù‚Ø§Ø¦Ù…Ø© IDs Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        stage:    Ø§Ù„Ù…Ø±Ø­Ù„Ø© (Ø§ÙØªØ±Ø§Ø¶ÙŠ: clustering)

    Returns:
        Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ÙØ¶Ø§ÙØ© ÙØ¹Ù„Ø§Ù‹
    """
    if not news_ids:
        return 0

    conn = get_conn()
    if not conn:
        return 0

    try:
        cur = conn.cursor()
        count = 0
        for nid in news_ids:
            cur.execute("""
                INSERT INTO news_pipeline_queue (news_id, stage, status, next_run_at)
                VALUES (%s, %s, 'pending', NOW())
                ON CONFLICT DO NOTHING
            """, (nid, stage))
            count += cur.rowcount
        conn.commit()
        cur.close()
        return count
    except Exception as e:
        logging.getLogger('queue').error(f"âŒ enqueue_batch failed: {e}")
        conn.rollback()
        return 0
    finally:
        conn.close()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Queue Worker Class
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PipelineStageWorker:
    """
    Worker Ù„Ù…Ø±Ø­Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„Ù€ pipeline.

    ÙŠØ³Ø­Ø¨ Ù…Ù‡Ø§Ù… Ù…Ù† news_pipeline_queue Ø¨Ù€ FOR UPDATE SKIP LOCKEDØŒ
    ÙŠÙ†ÙØ°Ù‡Ø§ØŒ Ø«Ù… ÙŠØ¹Ù…Ù„ enqueue Ù„Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹.
    """

    def __init__(self, stage: str):
        self.stage   = stage
        self.logger  = logging.getLogger(f'worker.{stage}')
        self.running = True
        self._jobs_done = 0
        self._lock   = threading.Lock()

        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù€ job function
        self.job_func: Optional[Callable] = self._import_job()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Job Import
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _import_job(self) -> Optional[Callable]:
        """Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù€ job function Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ø±Ø­Ù„Ø©"""
        try:
            if self.stage == 'clustering':
                from app.jobs.clustering_job import cluster_news
                return cluster_news

            elif self.stage == 'report_generation':
                from app.jobs.reports_job import generate_reports
                return generate_reports

            elif self.stage == 'image_generation':
                from app.jobs.image_generation_job import generate_images
                return generate_images

            else:
                self.logger.error(f"âŒ Unknown stage: {self.stage}")
                return None

        except ImportError as e:
            self.logger.error(f"âŒ Failed to import job for {self.stage}: {e}")
            return None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Queue Operations
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _fetch_task(self) -> Optional[Dict]:
        """
        Ø¬Ù„Ø¨ Ù…Ù‡Ù…Ø© Ù…Ø³ØªØ­Ù‚Ø© Ù…Ù† Ø§Ù„Ù€ queue Ù…Ø¹ locking.
        ÙŠØ³ØªØ®Ø¯Ù… FOR UPDATE SKIP LOCKED Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶ Ø¨ÙŠÙ† workers.
        """
        conn = get_conn()
        if not conn:
            return None

        try:
            cur = conn.cursor()

            cur.execute(f"""
                SELECT id, news_id, stage, attempt_count
                FROM news_pipeline_queue
                WHERE stage  = %s
                  AND status = 'pending'
                  AND next_run_at <= NOW()
                  AND (
                      locked_at IS NULL
                      OR locked_at < NOW() - INTERVAL '{LOCK_TIMEOUT} minutes'
                  )
                ORDER BY next_run_at ASC
                LIMIT 1
                FOR UPDATE SKIP LOCKED
            """, (self.stage,))

            row = cur.fetchone()
            if not row:
                cur.close()
                conn.close()
                return None

            task_id, news_id, stage, attempt_count = row

            # Ù‚ÙÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©
            now = datetime.now(timezone.utc)
            cur.execute("""
                UPDATE news_pipeline_queue
                SET status     = 'running',
                    locked_at  = %s,
                    locked_by  = %s,
                    started_at = %s
                WHERE id = %s
            """, (now, WORKER_ID, now, task_id))

            conn.commit()
            cur.close()
            conn.close()

            return {
                'id':            task_id,
                'news_id':       news_id,
                'stage':         stage,
                'attempt_count': attempt_count,
                'locked_at':     now,
            }

        except Exception as e:
            self.logger.error(f"âŒ Error fetching task: {e}")
            if conn:
                conn.rollback()
                conn.close()
            return None

    def _mark_done(self, task: Dict, result: str = None):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù‡Ù…Ø© ÙƒÙ…Ù†ØªÙ‡ÙŠØ© ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ù€ queue"""
        conn = get_conn()
        if not conn:
            return

        try:
            cur = conn.cursor()
            now = datetime.now(timezone.utc)

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            cur.execute("""
                UPDATE news_pipeline_queue
                SET status      = 'done',
                    locked_at   = NULL,
                    locked_by   = NULL,
                    finished_at = %s,
                    result      = %s
                WHERE id = %s
            """, (now, result, task['id']))

            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            next_stage = NEXT_STAGE.get(self.stage)
            if next_stage and task.get('news_id'):
                cur.execute("""
                    INSERT INTO news_pipeline_queue (news_id, stage, status, next_run_at)
                    VALUES (%s, %s, 'pending', NOW())
                    ON CONFLICT DO NOTHING
                """, (task['news_id'], next_stage))

                if cur.rowcount > 0:
                    self.logger.info(
                        f"â¡ï¸  Enqueued news #{task['news_id']} â†’ {next_stage}"
                    )

            conn.commit()
            cur.close()

        except Exception as e:
            self.logger.error(f"âŒ Error marking done: {e}")
            if conn:
                conn.rollback()
        finally:
            if conn:
                conn.close()

    def _mark_failed(self, task: Dict, error: str):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù‡Ù…Ø© ÙƒÙØ§Ø´Ù„Ø© Ù…Ø¹ retry backoff"""
        conn = get_conn()
        if not conn:
            return

        try:
            cur = conn.cursor()
            now = datetime.now(timezone.utc)
            attempt = task['attempt_count'] + 1

            if attempt >= MAX_ATTEMPTS:
                # ÙØ´Ù„ Ù†Ù‡Ø§Ø¦ÙŠ
                cur.execute("""
                    UPDATE news_pipeline_queue
                    SET status        = 'failed',
                        locked_at     = NULL,
                        locked_by     = NULL,
                        finished_at   = %s,
                        attempt_count = %s,
                        error_message = %s
                    WHERE id = %s
                """, (now, attempt, error[:1000], task['id']))

                self.logger.error(
                    f"âŒ Task #{task['id']} (news={task['news_id']}) "
                    f"permanently failed after {attempt} attempts"
                )

            else:
                # Ø¥Ø¹Ø§Ø¯Ø© Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹ backoff
                backoff = RETRY_BACKOFF.get(attempt, 15)
                next_run = now + timedelta(minutes=backoff)

                cur.execute("""
                    UPDATE news_pipeline_queue
                    SET status        = 'pending',
                        locked_at     = NULL,
                        locked_by     = NULL,
                        next_run_at   = %s,
                        attempt_count = %s,
                        error_message = %s
                    WHERE id = %s
                """, (next_run, attempt, error[:1000], task['id']))

                self.logger.warning(
                    f"âš ï¸  Task #{task['id']} failed (attempt {attempt}/{MAX_ATTEMPTS}), "
                    f"retry in {backoff}min"
                )

            conn.commit()
            cur.close()

        except Exception as e:
            self.logger.error(f"âŒ Error marking failed: {e}")
            if conn:
                conn.rollback()
        finally:
            if conn:
                conn.close()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Job Execution
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _execute(self, task: Dict) -> Dict:
        """
        ØªÙ†ÙÙŠØ° Ø§Ù„Ù€ job Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ù…Ø±Ø­Ù„Ø©.

        Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ù€ jobs Ø§Ù„Ø­Ø§Ù„ÙŠØ© (cluster_news, generate_reports, generate_images)
        ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ batch (ÙƒÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ØºÙŠØ± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©)ØŒ Ù…Ø´ Ø¹Ù„Ù‰ Ø®Ø¨Ø± ÙˆØ§Ø­Ø¯.
        Ù‡Ø°Ø§ Ù…Ù‚Ø¨ÙˆÙ„ Ù„Ø£Ù† Ø§Ù„Ù€ queue ØªØ¶Ù…Ù† Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„ØªØ³Ù„Ø³Ù„.
        """
        started = datetime.now(timezone.utc)

        self.logger.info(
            f"â–¶ï¸  [{self.stage}] Processing task #{task['id']} "
            f"(news={task['news_id']}, attempt={task['attempt_count']+1})"
        )

        try:
            result = self.job_func()
            finished = datetime.now(timezone.utc)
            duration = (finished - started).total_seconds()

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            if isinstance(result, dict):
                if result.get('error'):
                    return {
                        'success': False,
                        'error':   result['error'],
                        'result':  None,
                        'duration': duration,
                    }
                elif result.get('skipped'):
                    return {
                        'success': True,
                        'error':   None,
                        'result':  f"skipped: {result.get('reason', '')}",
                        'duration': duration,
                    }
                else:
                    summary = str(result.get(
                        'processed',
                        result.get('generated', result.get('count', 'done'))
                    ))
                    return {
                        'success': True,
                        'error':   None,
                        'result':  summary,
                        'duration': duration,
                    }
            else:
                return {
                    'success': True,
                    'error':   None,
                    'result':  str(result) if result else 'done',
                    'duration': duration,
                }

        except Exception as e:
            finished = datetime.now(timezone.utc)
            duration = (finished - started).total_seconds()
            self.logger.error(f"âŒ [{self.stage}] Job exception: {e}")
            traceback.print_exc()
            return {
                'success': False,
                'error':   str(e),
                'result':  None,
                'duration': duration,
            }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Main Loop
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(self):
        """Main worker loop - ÙŠØ´ØªØºÙ„ Ø­ØªÙ‰ ÙŠÙÙˆÙ‚Ù"""
        self.logger.info(f"ğŸš€ [{self.stage}] Worker started (id={WORKER_ID})")

        if not self.job_func:
            self.logger.error(f"âŒ [{self.stage}] No job function, exiting")
            return

        last_heartbeat = datetime.now()

        while self.running:
            try:
                task = self._fetch_task()

                if not task:
                    # Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‡Ø§Ù… - heartbeat ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
                    if (datetime.now() - last_heartbeat).total_seconds() >= 60:
                        self.logger.debug(
                            f"ğŸ’“ [{self.stage}] alive - {self._jobs_done} done"
                        )
                        last_heartbeat = datetime.now()
                    time.sleep(POLL_INTERVAL)
                    continue

                # ØªÙ†ÙÙŠØ° Ø§Ù„Ù€ job
                exec_result = self._execute(task)

                if exec_result['success']:
                    self._mark_done(task, result=exec_result['result'])
                    with self._lock:
                        self._jobs_done += 1
                    self.logger.info(
                        f"âœ… [{self.stage}] Task #{task['id']} done "
                        f"in {exec_result['duration']:.1f}s"
                    )
                else:
                    self._mark_failed(task, error=exec_result['error'])
                    self.logger.error(
                        f"âŒ [{self.stage}] Task #{task['id']} failed "
                        f"in {exec_result['duration']:.1f}s: {exec_result['error']}"
                    )

            except KeyboardInterrupt:
                self.logger.info(f"âš ï¸  [{self.stage}] Keyboard interrupt")
                break

            except Exception as e:
                self.logger.error(f"âŒ [{self.stage}] Loop error: {e}")
                traceback.print_exc()
                time.sleep(POLL_INTERVAL)

        self.logger.info(
            f"ğŸ›‘ [{self.stage}] Worker stopped - {self._jobs_done} jobs done"
        )

    def stop(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù€ worker Ø¨Ø´ÙƒÙ„ Ù†Ø¸ÙŠÙ"""
        self.running = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Multi-Stage Runner (All-in-One)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PipelineQueueRunner:
    """
    ÙŠØ´ØºÙ„ ÙƒÙ„ Ø§Ù„Ù€ workers ÙÙŠ threads Ù…Ù†ÙØµÙ„Ø© Ø¯Ø§Ø®Ù„ Ù†ÙØ³ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©.
    Ù…ÙÙŠØ¯ Ù„Ù„Ù€ development Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹Ù†Ø¯Ùƒ resource Ù…Ø­Ø¯ÙˆØ¯.

    Ù„Ù„Ù€ production: Ø´ØºÙ‘Ù„ ÙƒÙ„ worker ÙÙŠ process Ù…Ù†ÙØµÙ„.
    """

    STAGES = ['clustering', 'report_generation', 'image_generation']

    def __init__(self):
        self.logger  = logging.getLogger('pipeline.runner')
        self.workers = {}
        self.threads = {}
        self.running = True

    def start(self):
        """ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù€ workers"""
        self.logger.info("â•" * 70)
        self.logger.info("ğŸš€ Pipeline Queue Runner Starting")
        self.logger.info(f"   Stages: {' â†’ '.join(self.STAGES)}")
        self.logger.info(f"   Poll interval: {POLL_INTERVAL}s")
        self.logger.info(f"   Max attempts: {MAX_ATTEMPTS}")
        self.logger.info(f"   Lock timeout: {LOCK_TIMEOUT}min")
        self.logger.info("â•" * 70)

        # Setup signal handlers
        signal.signal(signal.SIGTERM, self._signal_handler)
        signal.signal(signal.SIGINT,  self._signal_handler)

        # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ workers
        for stage in self.STAGES:
            worker = PipelineStageWorker(stage)
            thread = threading.Thread(
                target=worker.run,
                name=f"worker-{stage}",
                daemon=True
            )
            self.workers[stage] = worker
            self.threads[stage] = thread
            thread.start()
            self.logger.info(f"   âœ… Started {stage} worker")

        self.logger.info("â•" * 70)

        # Ø§Ù†ØªØ¸Ø± Ø­ØªÙ‰ ÙŠÙÙˆÙ‚Ù
        try:
            while self.running:
                time.sleep(1)
                # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù€ threads Ù„Ø§ ØªØ²Ø§Ù„ ØªØ¹Ù…Ù„
                for stage, thread in self.threads.items():
                    if not thread.is_alive():
                        self.logger.warning(f"âš ï¸  {stage} thread died, restarting...")
                        worker = PipelineStageWorker(stage)
                        new_thread = threading.Thread(
                            target=worker.run,
                            name=f"worker-{stage}",
                            daemon=True
                        )
                        self.workers[stage] = worker
                        self.threads[stage] = new_thread
                        new_thread.start()

        except KeyboardInterrupt:
            pass

        self.stop()

    def stop(self):
        """Ø¥ÙŠÙ‚Ø§Ù ÙƒÙ„ Ø§Ù„Ù€ workers"""
        self.logger.info("âš ï¸  Stopping all pipeline workers...")
        self.running = False

        for stage, worker in self.workers.items():
            worker.stop()

        # Ø§Ù†ØªØ¸Ø± Ø§Ù„Ù€ threads
        for stage, thread in self.threads.items():
            thread.join(timeout=10)
            self.logger.info(f"   ğŸ›‘ {stage} worker stopped")

        self.logger.info("âœ… All pipeline workers stopped")

    def _signal_handler(self, signum, frame):
        self.logger.info(f"\nâš ï¸  Signal {signum} received, shutting down...")
        self.running = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Utility Functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def show_queue_stats():
    """Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù€ queue"""
    conn = get_conn()
    if not conn:
        print("âŒ Cannot connect to database")
        return

    try:
        cur = conn.cursor()

        print("\n" + "â•" * 60)
        print("ğŸ“Š Pipeline Queue Statistics")
        print("â•" * 60)

        cur.execute("""
            SELECT stage, status, COUNT(*), MIN(created_at), MAX(created_at)
            FROM news_pipeline_queue
            GROUP BY stage, status
            ORDER BY stage, status
        """)

        rows = cur.fetchall()
        if not rows:
            print("  (empty queue)")
        else:
            current_stage = None
            for stage, status, count, oldest, newest in rows:
                if stage != current_stage:
                    print(f"\n  ğŸ“Œ {stage}:")
                    current_stage = stage
                print(f"     {status:10s}: {count:5d}  (oldest: {oldest}, newest: {newest})")

        print("\n" + "â•" * 60)

        # Ø¢Ø®Ø± 10 Ù…Ù‡Ø§Ù…
        cur.execute("""
            SELECT id, news_id, stage, status, attempt_count,
                   created_at, finished_at, error_message
            FROM news_pipeline_queue
            ORDER BY created_at DESC
            LIMIT 10
        """)

        rows = cur.fetchall()
        print("ğŸ“‹ Last 10 Queue Items:")
        for row in rows:
            qid, nid, stage, status, attempts, created, finished, error = row
            duration = ""
            if finished and created:
                duration = f" ({(finished - created).total_seconds():.1f}s)"
            err = f" âš ï¸ {error[:50]}" if error else ""
            print(f"  #{qid:6d} news={nid} [{stage}] {status} (x{attempts}){duration}{err}")

        print("â•" * 60 + "\n")

        cur.close()
    except Exception as e:
        print(f"âŒ Error: {e}")
    finally:
        conn.close()


def reset_stuck_tasks(stage: str = None, minutes: int = 60):
    """
    Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø¹Ø§Ù„Ù‚Ø© (running Ù„ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø©)

    Args:
        stage:   Ù…Ø±Ø­Ù„Ø© Ù…Ø­Ø¯Ø¯Ø© Ø£Ùˆ None Ù„Ù„ÙƒÙ„
        minutes: Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚ Ù„Ù„Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ù…Ù‡Ù…Ø© Ø¹Ø§Ù„Ù‚Ø©
    """
    conn = get_conn()
    if not conn:
        return

    try:
        cur = conn.cursor()

        if stage:
            cur.execute("""
                UPDATE news_pipeline_queue
                SET status    = 'pending',
                    locked_at = NULL,
                    locked_by = NULL
                WHERE status = 'running'
                  AND stage  = %s
                  AND locked_at < NOW() - INTERVAL '%s minutes'
            """, (stage, minutes))
        else:
            cur.execute(f"""
                UPDATE news_pipeline_queue
                SET status    = 'pending',
                    locked_at = NULL,
                    locked_by = NULL
                WHERE status = 'running'
                  AND locked_at < NOW() - INTERVAL '{minutes} minutes'
            """)

        count = cur.rowcount
        conn.commit()
        cur.close()
        print(f"âœ… Reset {count} stuck tasks")

    except Exception as e:
        print(f"âŒ Error: {e}")
        conn.rollback()
    finally:
        conn.close()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Entry Point
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description='Pipeline Queue Workers - Real-Time Event-Driven Processing'
    )
    parser.add_argument(
        '--stage',
        choices=['clustering', 'report_generation', 'image_generation'],
        help='ØªØ´ØºÙŠÙ„ worker Ù„Ù…Ø±Ø­Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·'
    )
    parser.add_argument(
        '--all-in-one',
        action='store_true',
        help='ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù€ workers ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© (Ù„Ù„ØªØ·ÙˆÙŠØ±)'
    )
    parser.add_argument(
        '--stats',
        action='store_true',
        help='Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù€ queue'
    )
    parser.add_argument(
        '--reset-stuck',
        action='store_true',
        help='Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø¹Ø§Ù„Ù‚Ø©'
    )
    parser.add_argument(
        '--enqueue',
        type=int,
        metavar='NEWS_ID',
        help='Ø¥Ø¶Ø§ÙØ© Ø®Ø¨Ø± ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù„Ù„Ù€ queue (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)'
    )

    args = parser.parse_args()

    if args.stats:
        show_queue_stats()
        return

    if args.reset_stuck:
        reset_stuck_tasks()
        return

    if args.enqueue:
        success = enqueue(args.enqueue, 'clustering')
        if success:
            print(f"âœ… News #{args.enqueue} enqueued for clustering")
        else:
            print(f"âš ï¸  News #{args.enqueue} already in queue or failed")
        return

    if args.stage:
        # ØªØ´ØºÙŠÙ„ worker ÙˆØ§Ø­Ø¯
        worker = PipelineStageWorker(args.stage)

        def _stop(signum, frame):
            print(f"\nâš ï¸  Signal {signum}, stopping...")
            worker.stop()

        signal.signal(signal.SIGTERM, _stop)
        signal.signal(signal.SIGINT,  _stop)

        worker.run()

    else:
        # ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù€ workers (all-in-one Ø£Ùˆ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ)
        runner = PipelineQueueRunner()
        runner.start()


if __name__ == '__main__':
    main()
