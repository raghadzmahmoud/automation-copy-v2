#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ”„ Continuous Pipeline Scheduler
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø¨Ø³ÙŠØ· ÙˆÙ…Ø¨Ø§Ø´Ø±:
- ÙƒÙ„ task ØªØ®Ù„Øµ â†’ Ø§Ù„Ù„ÙŠ Ø¨Ø¹Ø¯Ù‡Ø§ ØªØ¨Ø¯Ø£
- Ù„Ù…Ø§ ÙŠØ®Ù„Øµ Ø§Ù„ÙƒÙ„ â†’ Ù†Ø¹ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
- Ù„Ø§ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø¹ÙŠØ¯ cron
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
import certifi
import os
os.environ["SSL_CERT_FILE"] = certifi.where()

import psycopg2
import logging
import time
import threading
from datetime import datetime, timezone
from typing import Dict, List, Optional, Callable

from settings import DB_CONFIG

logger = logging.getLogger(__name__)

# ============================================
# ğŸ“‹ Pipeline Configuration
# ============================================

# ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù‡Ø§Ù… ÙÙŠ Ø§Ù„Ù€ Pipeline
PIPELINE_ORDER = [
    'scraping',           # 1. Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
    'clustering',         # 2. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
    'report_generation',  # 3. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
    'image_generation',   # 4. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±
    'audio_generation',   # 5. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª
    'bulletin_generation', # 6. Ø§Ù„Ù†Ø´Ø±Ø©
    'digest_generation',  # 7. Ø§Ù„Ù…ÙˆØ¬Ø²
    'social_media_generation',  # 8. Ø³ÙˆØ´ÙŠØ§Ù„ Ù…ÙŠØ¯ÙŠØ§ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
]

# Ø§Ù„ÙØªØ±Ø© Ø¨ÙŠÙ† ÙƒÙ„ Ø¯ÙˆØ±Ø© pipeline ÙƒØ§Ù…Ù„Ø© (Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ)
PIPELINE_COOLDOWN = 60  # Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø¹Ø¯ Ù…Ø§ ÙŠØ®Ù„Øµ Ø§Ù„ÙƒÙ„

# Task functions mapping
TASK_FUNCTIONS: Dict[str, Callable] = {}

# Pipeline state
pipeline_running = False
pipeline_thread = None
stop_flag = threading.Event()


# ============================================
# ğŸ“ Task Registration
# ============================================

def register_task(task_type: str, func: Callable):
    """Register a task function"""
    TASK_FUNCTIONS[task_type] = func
    logger.info(f"ğŸ“ Registered: {task_type}")


def register_all_tasks():
    """Register all task functions"""
    
    def scraping_task():
        from app.jobs.scraper_job import scrape_news
        return scrape_news()
    
    def clustering_task():
        from app.jobs.clustering_job import cluster_news
        return cluster_news()
    
    def report_generation_task():
        from app.jobs.reports_job import generate_reports
        return generate_reports()
    
    def image_generation_task():
        from app.jobs.image_generation_job import generate_images
        return generate_images()
    
    def audio_generation_task():
        from app.jobs.audio_generation_job import generate_audio
        return generate_audio()
    
    def bulletin_task():
        from app.jobs.bulletin_digest_job import generate_bulletin_job
        return generate_bulletin_job()
    
    def digest_task():
        from app.jobs.bulletin_digest_job import generate_digest_job
        return generate_digest_job()
    
    def social_media_task():
        from app.jobs.social_media_job import generate_social_media_content
        return generate_social_media_content()
    
    register_task('scraping', scraping_task)
    register_task('clustering', clustering_task)
    register_task('report_generation', report_generation_task)
    register_task('image_generation', image_generation_task)
    register_task('audio_generation', audio_generation_task)
    register_task('bulletin_generation', bulletin_task)
    register_task('digest_generation', digest_task)
    register_task('social_media_generation', social_media_task)


# ============================================
# ğŸ—„ï¸ Database Functions
# ============================================

def get_db_connection():
    """Create database connection"""
    try:
        return psycopg2.connect(**DB_CONFIG)
    except Exception as e:
        logger.error(f"âŒ Database connection failed: {e}")
        return None


def get_active_tasks_from_db() -> List[str]:
    """
    Get ordered list of active tasks from database
    Returns tasks in execution order
    """
    conn = get_db_connection()
    if not conn:
        return PIPELINE_ORDER  # fallback to default
    
    try:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT task_type 
            FROM scheduled_tasks
            WHERE status = 'active'
            ORDER BY COALESCE(execution_order, 99), id
        """)
        
        active_tasks = [row[0] for row in cursor.fetchall()]
        cursor.close()
        conn.close()
        
        # Filter to only tasks we have functions for
        return [t for t in active_tasks if t in TASK_FUNCTIONS]
        
    except Exception as e:
        logger.error(f"âŒ Error fetching tasks: {e}")
        if conn:
            conn.close()
        return PIPELINE_ORDER


def update_task_last_run(task_type: str):
    """Update last_run_at for a task"""
    conn = get_db_connection()
    if not conn:
        return
    
    try:
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE scheduled_tasks 
            SET last_run_at = %s
            WHERE task_type = %s
        """, (datetime.now(timezone.utc), task_type))
        
        conn.commit()
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"âŒ Error updating last_run: {e}")
        if conn:
            conn.rollback()
            conn.close()


def log_task_execution(task_type: str, status: str, duration: float = 0, error: str = None):
    """Log task execution"""
    conn = get_db_connection()
    if not conn:
        return
    
    try:
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT id FROM scheduled_tasks WHERE task_type = %s",
            (task_type,)
        )
        row = cursor.fetchone()
        if not row:
            conn.close()
            return
        
        cursor.execute("""
            INSERT INTO scheduled_task_logs 
            (scheduled_task_id, status, execution_time_seconds, error_message, executed_at)
            VALUES (%s, %s, %s, %s, %s)
        """, (row[0], status, duration, error, datetime.now(timezone.utc)))
        
        conn.commit()
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"âŒ Error logging: {e}")
        if conn:
            conn.close()


def log_pipeline_cycle(cycle_number: int, total_duration: float, results: Dict):
    """Log complete pipeline cycle"""
    conn = get_db_connection()
    if not conn:
        return
    
    try:
        cursor = conn.cursor()
        
        # Get or create pipeline task
        cursor.execute(
            "SELECT id FROM scheduled_tasks WHERE task_type = 'processing_pipeline'"
        )
        row = cursor.fetchone()
        if row:
            cursor.execute("""
                INSERT INTO scheduled_task_logs 
                (scheduled_task_id, status, execution_time_seconds, result, executed_at)
                VALUES (%s, %s, %s, %s, %s)
            """, (row[0], 'completed', total_duration, 
                  f"Cycle #{cycle_number}: {len(results)} tasks", 
                  datetime.now(timezone.utc)))
            
            conn.commit()
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"âŒ Error logging cycle: {e}")
        if conn:
            conn.close()


# ============================================
# âš™ï¸ Task Execution
# ============================================

def execute_task(task_type: str) -> Dict:
    """
    Execute a single task
    Returns: {'success': bool, 'duration': float, 'error': str|None}
    """
    if task_type not in TASK_FUNCTIONS:
        logger.error(f"âŒ Unknown task: {task_type}")
        return {'success': False, 'duration': 0, 'error': 'Unknown task'}
    
    logger.info(f"â–¶ï¸ Starting: {task_type}")
    start_time = datetime.now()
    
    try:
        # Execute the task
        result = TASK_FUNCTIONS[task_type]()
        
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f"âœ… {task_type} completed in {duration:.2f}s")
        
        # Update database
        update_task_last_run(task_type)
        log_task_execution(task_type, 'completed', duration)
        
        return {
            'success': True,
            'duration': duration,
            'error': None,
            'result': result
        }
        
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        error_msg = str(e)
        logger.error(f"âŒ {task_type} failed: {error_msg}")
        
        import traceback
        traceback.print_exc()
        
        log_task_execution(task_type, 'failed', duration, error_msg)
        
        return {
            'success': False,
            'duration': duration,
            'error': error_msg
        }


# ============================================
# ğŸ”„ Pipeline Execution
# ============================================

def run_pipeline_cycle(cycle_number: int) -> Dict:
    """
    Run one complete pipeline cycle
    All tasks execute in sequence
    """
    logger.info("=" * 70)
    logger.info(f"ğŸ”„ Pipeline Cycle #{cycle_number} starting...")
    logger.info("=" * 70)
    
    cycle_start = datetime.now()
    results = {}
    
    # Get active tasks in order
    tasks = get_active_tasks_from_db()
    
    # Filter out processing_pipeline (we don't want to run the old one)
    tasks = [t for t in tasks if t != 'processing_pipeline']
    
    logger.info(f"ğŸ“‹ Tasks to execute: {' â†’ '.join(tasks)}")
    logger.info("-" * 70)
    
    for i, task_type in enumerate(tasks, 1):
        # Check if we should stop
        if stop_flag.is_set():
            logger.info("â¹ï¸ Pipeline stopped by user")
            break
        
        logger.info(f"[{i}/{len(tasks)}] {task_type}")
        
        # Execute task
        result = execute_task(task_type)
        results[task_type] = result
        
        # Small delay between tasks to prevent overwhelming
        if result['success'] and i < len(tasks):
            time.sleep(2)
    
    # Calculate total duration
    total_duration = (datetime.now() - cycle_start).total_seconds()
    
    # Log cycle completion
    log_pipeline_cycle(cycle_number, total_duration, results)
    
    # Print summary
    logger.info("=" * 70)
    logger.info(f"ğŸ Pipeline Cycle #{cycle_number} completed in {total_duration:.2f}s")
    logger.info("-" * 70)
    
    successful = sum(1 for r in results.values() if r['success'])
    failed = len(results) - successful
    
    for task, result in results.items():
        status = "âœ…" if result['success'] else "âŒ"
        logger.info(f"   {status} {task}: {result['duration']:.2f}s")
    
    logger.info("-" * 70)
    logger.info(f"   Total: {successful} succeeded, {failed} failed")
    logger.info("=" * 70)
    
    return {
        'cycle': cycle_number,
        'duration': total_duration,
        'results': results,
        'successful': successful,
        'failed': failed
    }


def pipeline_loop():
    """
    Main pipeline loop - runs continuously
    """
    global pipeline_running
    
    cycle_number = 0
    
    while not stop_flag.is_set():
        cycle_number += 1
        
        try:
            # Run one cycle
            run_pipeline_cycle(cycle_number)
            
            # Cooldown before next cycle
            if not stop_flag.is_set():
                logger.info(f"ğŸ˜´ Cooling down for {PIPELINE_COOLDOWN}s before next cycle...")
                
                # Sleep in small chunks to allow quick stop
                for _ in range(PIPELINE_COOLDOWN):
                    if stop_flag.is_set():
                        break
                    time.sleep(1)
                    
        except Exception as e:
            logger.error(f"âŒ Pipeline cycle error: {e}")
            import traceback
            traceback.print_exc()
            
            # Wait before retrying
            time.sleep(30)
    
    pipeline_running = False
    logger.info("â¹ï¸ Pipeline loop ended")


# ============================================
# ğŸš€ Pipeline Control
# ============================================

def start_pipeline():
    """Start the continuous pipeline"""
    global pipeline_running, pipeline_thread, stop_flag
    
    if pipeline_running:
        logger.warning("âš ï¸ Pipeline already running!")
        return False
    
    logger.info("ğŸš€ Starting Continuous Pipeline...")
    
    # Register tasks
    register_all_tasks()
    
    # Show registered tasks
    logger.info(f"ğŸ“‹ Registered tasks: {list(TASK_FUNCTIONS.keys())}")
    
    # Reset stop flag
    stop_flag.clear()
    
    # Start pipeline thread
    pipeline_running = True
    pipeline_thread = threading.Thread(target=pipeline_loop, daemon=True)
    pipeline_thread.start()
    
    logger.info("âœ… Pipeline started!")
    return True


def stop_pipeline():
    """Stop the pipeline gracefully"""
    global pipeline_running, stop_flag
    
    if not pipeline_running:
        logger.warning("âš ï¸ Pipeline not running!")
        return False
    
    logger.info("â¹ï¸ Stopping pipeline...")
    stop_flag.set()
    
    # Wait for thread to finish
    if pipeline_thread:
        pipeline_thread.join(timeout=30)
    
    pipeline_running = False
    logger.info("âœ… Pipeline stopped!")
    return True


def get_pipeline_status() -> Dict:
    """Get current pipeline status"""
    return {
        'running': pipeline_running,
        'tasks': list(TASK_FUNCTIONS.keys()),
        'order': PIPELINE_ORDER,
        'cooldown': PIPELINE_COOLDOWN
    }


# ============================================
# ğŸ”§ Manual Controls
# ============================================

def run_single_task(task_type: str) -> Dict:
    """Manually run a single task"""
    if task_type not in TASK_FUNCTIONS:
        # Try to register if not registered
        register_all_tasks()
    
    return execute_task(task_type)


def run_single_cycle() -> Dict:
    """Manually run a single pipeline cycle"""
    register_all_tasks()
    return run_pipeline_cycle(0)


# ============================================
# ğŸš€ Main
# ============================================

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    logger.info("=" * 70)
    logger.info("ğŸ”„ Continuous Pipeline Scheduler")
    logger.info("=" * 70)
    logger.info(f"ğŸ“‹ Pipeline order: {' â†’ '.join(PIPELINE_ORDER)}")
    logger.info(f"â±ï¸ Cooldown between cycles: {PIPELINE_COOLDOWN}s")
    logger.info("=" * 70)
    
    start_pipeline()
    
    try:
        # Keep main thread alive
        while pipeline_running:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ Shutting down...")
        stop_pipeline()