#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ§ª Test Manual Scraper
Ù…Ù„Ù ØªØ¬Ø±Ø¨Ø© Ù„Ù„Ù€ Manual URL Scraper
"""

import sys
import os

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø±
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ============================================
# ğŸ§ª Test 1: Source Detector
# ============================================
def test_source_detector():
    """ØªØ¬Ø±Ø¨Ø© ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø±"""
    print("\n" + "="*60)
    print("ğŸ§ª Test 1: Source Detector")
    print("="*60)
    
    from app.services.ingestion.source_detector import detect_source
    
    test_urls = [
        "https://www.aljazeera.net/news/2024/1/15/example",
        "https://t.me/TestChannel",
        "https://t.me/TestChannel/12345",
        "https://example.com/feed.xml",
        "www.example.com/news",
        "invalid",
    ]
    
    for url in test_urls:
        info = detect_source(url)
        status = "âœ…" if info.is_valid else "âŒ"
        print(f"\n{status} {url}")
        print(f"   Type: {info.source_type.value}")
        if info.telegram_username:
            print(f"   Telegram: @{info.telegram_username}")
        if info.error_message:
            print(f"   Error: {info.error_message}")
    
    print("\nâœ… Source Detector Test Complete!")
    return True


# ============================================
# ğŸ§ª Test 2: Web Scraper (Ø¨Ø¯ÙˆÙ† DB)
# ============================================
def test_web_scraper(url: str = None):
    """ØªØ¬Ø±Ø¨Ø© Ø³Ø­Ø¨ Ù…Ø­ØªÙˆÙ‰ ØµÙØ­Ø©"""
    print("\n" + "="*60)
    print("ğŸ§ª Test 2: Web Scraper")
    print("="*60)
    
    from app.services.ingestion.web_scraper import scrape_url
    
    test_url = url or "https://www.maannews.net/"
    print(f"\nğŸ”— URL: {test_url}")
    print("â³ Scraping...")
    
    result = scrape_url(test_url)
    
    if result.success:
        print(f"\nâœ… Success!")
        print(f"ğŸ“° Title: {result.title[:80]}..." if result.title else "   No title")
        print(f"ğŸ“ Content Length: {len(result.clean_text)} chars")
        print(f"ğŸ–¼ï¸ Images Found: {len(result.images)}")
        print(f"ğŸ¬ Videos Found: {len(result.videos)}")
        
        if result.clean_text:
            print(f"\nğŸ“„ First 500 chars of content:")
            print("-" * 40)
            print(result.clean_text[:500])
            print("-" * 40)
        
        if result.images:
            print(f"\nğŸ–¼ï¸ First 3 images:")
            for i, img in enumerate(result.images[:3], 1):
                print(f"   [{i}] {img[:80]}...")
    else:
        print(f"\nâŒ Failed: {result.error_message}")
    
    return result.success


# ============================================
# ğŸ§ª Test 3: Content Extractor (ÙŠØ­ØªØ§Ø¬ API Key)
# ============================================
def test_content_extractor():
    """ØªØ¬Ø±Ø¨Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø§Ù„Ù€ AI"""
    print("\n" + "="*60)
    print("ğŸ§ª Test 3: Content Extractor (AI)")
    print("="*60)
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† API Key
    from settings import GEMINI_API_KEY, GEMINI_EXTRACTION_MODEL
    
    if not GEMINI_API_KEY:
        print("\nâŒ GEMINI_API_KEY not set in .env")
        return False
    
    print(f"\nğŸ¤– Using Model: {GEMINI_EXTRACTION_MODEL}")
    
    from app.services.ingestion.content_extractor import ContentExtractor
    
    # Ù…Ø­ØªÙˆÙ‰ ØªØ¬Ø±ÙŠØ¨ÙŠ
    test_content = """
    Ø£Ø¹Ù„Ù†Øª Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ© Ø§Ù„ÙŠÙˆÙ… Ø¹Ù† Ø®Ø·Ø© Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©
    
    Ø±Ø§Ù… Ø§Ù„Ù„Ù‡ - Ø£Ø¹Ù„Ù† Ø±Ø¦ÙŠØ³ Ø§Ù„ÙˆØ²Ø±Ø§Ø¡ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ Ù…Ø­Ù…Ø¯ Ù…ØµØ·ÙÙ‰ Ø¹Ù† Ø®Ø·Ø© Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø´Ø§Ù…Ù„Ø© 
    ØªÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹ÙŠØ´ÙŠ Ù„Ù„Ù…ÙˆØ§Ø·Ù†ÙŠÙ† ÙÙŠ Ø§Ù„Ø¶ÙØ© Ø§Ù„ØºØ±Ø¨ÙŠØ© ÙˆÙ‚Ø·Ø§Ø¹ ØºØ²Ø©.
    ÙˆÙ‚Ø§Ù„ ÙÙŠ Ù…Ø¤ØªÙ…Ø± ØµØ­ÙÙŠ Ø¥Ù† Ø§Ù„Ø®Ø·Ø© ØªØªØ¶Ù…Ù† Ù…Ø´Ø§Ø±ÙŠØ¹ ØªÙ†Ù…ÙˆÙŠØ© ÙˆØªØ´ØºÙŠÙ„ÙŠØ©.
    
    ---
    
    Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ÙŠØªØ£Ù‡Ù„ Ù„ÙƒØ£Ø³ Ø¢Ø³ÙŠØ§
    
    ØªØ£Ù‡Ù„ Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ Ù„ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø¥Ù„Ù‰ Ù†Ù‡Ø§Ø¦ÙŠØ§Øª ÙƒØ£Ø³ Ø¢Ø³ÙŠØ§ 2027 
    Ø¨Ø¹Ø¯ ÙÙˆØ²Ù‡ Ø¹Ù„Ù‰ Ù†Ø¸ÙŠØ±Ù‡ Ø§Ù„Ù„Ø¨Ù†Ø§Ù†ÙŠ Ø¨Ù†ØªÙŠØ¬Ø© 2-0 ÙÙŠ Ø§Ù„ØªØµÙÙŠØ§Øª Ø§Ù„Ù…Ø¤Ù‡Ù„Ø©.
    Ø³Ø¬Ù„ Ø§Ù„Ù‡Ø¯ÙÙŠÙ† Ø§Ù„Ù„Ø§Ø¹Ø¨Ø§Ù† Ù…Ø­Ù…Ø¯ Ø³Ø§Ù„Ù… ÙˆØ£Ø­Ù…Ø¯ Ø£Ø¨Ùˆ Ù†Ø§Ù‡ÙŠØ©.
    """
    
    print("\nâ³ Extracting news with AI...")
    
    extractor = ContentExtractor()
    result = extractor.extract_news(
        content=test_content,
        source_url="https://www.maannews.net/"
    )
    
    if result.success:
        print(f"\nâœ… Extracted {result.total_extracted} news items!")
        
        for i, news in enumerate(result.news_items, 1):
            print(f"\n--- News #{i} ---")
            print(f"ğŸ“Œ Title: {news.title}")
            print(f"ğŸ“ Category: {news.category}")
            print(f"ğŸ·ï¸ Tags: {', '.join(news.tags[:5])}")
            print(f"ğŸ“ Content: {news.content[:100]}...")
    else:
        print(f"\nâŒ Failed: {result.error_message}")
    
    return result.success


# ============================================
# ğŸ§ª Test 4: Full Pipeline (Manual Scraper)
# ============================================
def test_full_pipeline(url: str = None, save_to_db: bool = False):
    """ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù€ Pipeline Ø§Ù„ÙƒØ§Ù…Ù„"""
    print("\n" + "="*60)
    print("ğŸ§ª Test 4: Full Pipeline (Manual Scraper)")
    print("="*60)
    
    from app.services.ingestion.manual_scraper import ManualScraper
    
    test_url = url or "https://www.maannews.net/"
    
    print(f"\nğŸ”— URL: {test_url}")
    print(f"ğŸ’¾ Save to DB: {save_to_db}")
    
    scraper = ManualScraper(auto_save=save_to_db)
    result = scraper.scrape_url(test_url)
    
    if result.success:
        print(f"\n" + "="*60)
        print(f"âœ… SUCCESS!")
        print(f"="*60)
        print(f"ğŸ“° News Extracted: {result.news_extracted}")
        print(f"ğŸ’¾ News Saved: {result.news_saved}")
        print(f"â±ï¸ Time: {result.processing_time_seconds:.2f}s")
        
        print(f"\nğŸ“‹ Extracted News:")
        for i, news in enumerate(result.news_items[:5], 1):  # Ø£ÙˆÙ„ 5
            print(f"\n[{i}] {news['title'][:60]}...")
            print(f"    ğŸ“ Category: {news.get('category_name', news.get('category_id', 'N/A'))}")
            print(f"    ğŸ·ï¸ Tags: {news.get('tags', '')[:50]}...")
    else:
        print(f"\nâŒ Failed: {result.error_message}")
    
    return result.success


# ============================================
# ğŸš€ Main
# ============================================
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Test Manual Scraper')
    parser.add_argument('--url', '-u', help='URL to test', default=None)
    parser.add_argument('--test', '-t', 
                       choices=['detector', 'scraper', 'extractor', 'full', 'all'],
                       default='all',
                       help='Which test to run')
    parser.add_argument('--save', '-s', action='store_true', 
                       help='Save to database (default: False)')
    
    args = parser.parse_args()
    
    print("\n" + "ğŸ§ª"*30)
    print("     MANUAL SCRAPER TEST SUITE")
    print("ğŸ§ª"*30)
    
    results = {}
    
    try:
        if args.test in ['detector', 'all']:
            results['Source Detector'] = test_source_detector()
        
        if args.test in ['scraper', 'all']:
            results['Web Scraper'] = test_web_scraper(args.url)
        
        if args.test in ['extractor', 'all']:
            results['Content Extractor'] = test_content_extractor()
        
        if args.test in ['full', 'all']:
            results['Full Pipeline'] = test_full_pipeline(args.url, args.save)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
    
    # Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\n" + "="*60)
    print("ğŸ“Š TEST SUMMARY")
    print("="*60)
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"   {test_name}: {status}")
    print("="*60)