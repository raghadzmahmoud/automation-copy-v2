#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸš€ URL Scraper - Command Line Tool
Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ø§Ù„Ù€ Database

Usage:
    python scrape_url.py <URL>
    python scrape_url.py <URL> --max 10
    python scrape_url.py <URL> --no-save
    
Examples:
    python scrape_url.py "https://www.example.com/news"
    python scrape_url.py "https://www.example.com" --max 5
    python scrape_url.py "https://www.example.com" --no-save --max 3
"""

import sys
import os
import argparse

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø±
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


def main():
    parser = argparse.ArgumentParser(
        description='ğŸ“¥ Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø±Ø§Ø¨Ø· ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ø§Ù„Ù€ Database',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scrape_url.py "https://www.example.com/news"
  python scrape_url.py "https://www.example.com" --max 5
  python scrape_url.py "https://www.example.com" --no-save
        """
    )
    
    parser.add_argument(
        'url',
        help='Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø±Ø§Ø¯ Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù†Ù‡'
    )
    
    parser.add_argument(
        '--max', '-m',
        type=int,
        default=10,
        help='Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 10)'
    )
    
    parser.add_argument(
        '--no-save',
        action='store_true',
        help='Ø¹Ø¯Ù… Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù€ Database (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙÙ‚Ø·)'
    )
    
    parser.add_argument(
        '--crawl',
        action='store_true',
        help='Ø¥Ø¬Ø¨Ø§Ø± ÙˆØ¶Ø¹ Ø§Ù„Ø²Ø­Ù (Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…)'
    )
    
    parser.add_argument(
        '--single',
        action='store_true',
        help='Ø¥Ø¬Ø¨Ø§Ø± ÙˆØ¶Ø¹ Ø§Ù„ØµÙØ­Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø© (Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ø­Ø¯)'
    )
    
    parser.add_argument(
        '--lang', '-l',
        type=int,
        default=1,
        help='ID Ø§Ù„Ù„ØºØ© (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 1 = Ø¹Ø±Ø¨ÙŠ)'
    )
    
    args = parser.parse_args()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·
    if not args.url:
        print("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø·!")
        parser.print_help()
        sys.exit(1)
    
    # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
    try:
        from app.services.ingestion.manual_scraper import ManualScraper
    except ImportError as e:
        print(f"âŒ Error importing modules: {e}")
        print("\nØªØ£ÙƒØ¯ Ø£Ù†Ùƒ ÙÙŠ Ù…Ø¬Ù„Ø¯ backend ÙˆØ£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…ÙˆØ¬ÙˆØ¯Ø©:")
        print("  - app/services/ingestion/manual_scraper.py")
        print("  - app/services/ingestion/news_crawler.py")
        print("  - app/services/ingestion/content_extractor.py")
        sys.exit(1)
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³Ø­Ø¨
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ“¥ URL News Scraper                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  URL: {args.url[:50]}{'...' if len(args.url) > 50 else ''}
â•‘  Max Articles: {args.max}
â•‘  Save to DB: {not args.no_save}
â•‘  Language ID: {args.lang}
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    scraper = ManualScraper(
        auto_save=not args.no_save,
        max_articles=args.max,
        default_language_id=args.lang
    )
    
    result = scraper.scrape_url(
        url=args.url,
        force_crawl=args.crawl,
        force_single=args.single
    )
    
    # Ø§Ù„Ù†ØªÙŠØ¬Ø©
    if result.success:
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      âœ… SUCCESS                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“° News Extracted: {result.news_extracted}
â•‘  ğŸ’¾ News Saved: {result.news_saved}
â•‘  ğŸ•·ï¸ Pages Crawled: {result.pages_crawled}
â•‘  â±ï¸ Time: {result.processing_time_seconds:.2f}s
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        if result.news_items:
            print("\nğŸ“‹ Extracted News:")
            print("â”€" * 60)
            for i, news in enumerate(result.news_items[:5], 1):
                print(f"\n[{i}] ğŸ“Œ {news.get('title', 'No Title')[:60]}...")
                print(f"    ğŸ“ Category ID: {news.get('category_id', 'N/A')}")
                print(f"    ğŸ·ï¸ Tags: {news.get('tags', '')[:40]}...")
            
            if len(result.news_items) > 5:
                print(f"\n    ... Ùˆ {len(result.news_items) - 5} Ø£Ø®Ø¨Ø§Ø± Ø£Ø®Ø±Ù‰")
        
        sys.exit(0)
    else:
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      âŒ FAILED                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Error: {result.error_message[:50] if result.error_message else 'Unknown error'}
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        sys.exit(1)


if __name__ == "__main__":
    main()