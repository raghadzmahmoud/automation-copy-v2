# ğŸ³ Docker Compose - Hybrid Architecture
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ—ï¸  Hybrid Architecture:
#
#   Cron Worker (worker.py):
#     - scraping          â†’ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ
#     - broadcast_generation â†’ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨Ø« Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ
#
#   Pipeline Queue Workers (pipeline_queue_workers.py):
#     - clustering_worker    â†’ real-time per news
#     - report_worker        â†’ real-time per news
#     - image_worker         â†’ real-time per news
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

version: '3.8'

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Shared environment (ÙŠÙØ³ØªØ®Ø¯Ù… ÙÙŠ ÙƒÙ„ Ø§Ù„Ù€ services)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
x-common-env: &common-env
  DB_NAME: ${DB_NAME:-automation_db}
  DB_USER: ${DB_USER:-postgres}
  DB_PASSWORD: ${DB_PASSWORD:-password}
  DB_HOST: ${DB_HOST:-host.docker.internal}
  DB_PORT: ${DB_PORT:-5432}
  ENVIRONMENT: development
  DEBUG: "True"
  LOG_LEVEL: INFO
  GEMINI_API_KEY: ${GEMINI_API_KEY}
  AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
  AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
  FB_GAZA_ACCESS_TOKEN: ${FB_GAZA_ACCESS_TOKEN}
  TG_BOT_TOKEN: ${TG_BOT_TOKEN}
  PYTHONPATH: /app
  PYTHONUNBUFFERED: "1"

x-common-build: &common-build
  build:
    context: ./backend
    dockerfile: Dockerfile.worker
  volumes:
    - ./backend:/app
    - worker-logs:/app/logs
  restart: unless-stopped
  networks:
    - automation-network

services:

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # â° Cron Worker - scraping + broadcast_generation
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  cron-worker:
    <<: *common-build
    container_name: automation-cron-worker
    command: python worker.py
    environment:
      <<: *common-env
      WORKER_TYPE: cron
      MAX_WORKERS: "2"
      WORKER_POLL_INTERVAL: "5"
    healthcheck:
      test: ["CMD", "python", "-c", "import psycopg2; print('ok')"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ¯ Clustering Worker - real-time queue-based
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  clustering-worker:
    <<: *common-build
    container_name: automation-clustering-worker
    command: python pipeline_queue_workers.py --stage clustering
    environment:
      <<: *common-env
      QUEUE_POLL_INTERVAL: "2"
      QUEUE_MAX_ATTEMPTS: "3"
    healthcheck:
      test: ["CMD", "python", "-c", "import psycopg2; print('ok')"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ“ Report Worker - real-time queue-based
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  report-worker:
    <<: *common-build
    container_name: automation-report-worker
    command: python pipeline_queue_workers.py --stage report_generation
    environment:
      <<: *common-env
      QUEUE_POLL_INTERVAL: "2"
      QUEUE_MAX_ATTEMPTS: "3"
    healthcheck:
      test: ["CMD", "python", "-c", "import psycopg2; print('ok')"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ¨ Image Worker - real-time queue-based
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  image-worker:
    <<: *common-build
    container_name: automation-image-worker
    command: python pipeline_queue_workers.py --stage image_generation
    environment:
      <<: *common-env
      QUEUE_POLL_INTERVAL: "2"
      QUEUE_MAX_ATTEMPTS: "3"
    healthcheck:
      test: ["CMD", "python", "-c", "import psycopg2; print('ok')"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ”„ Legacy Worker (Ù„Ù„ØªÙˆØ§ÙÙ‚ - ÙŠÙ…ÙƒÙ† Ø¥Ø²Ø§Ù„ØªÙ‡ Ù„Ø§Ø­Ù‚Ø§Ù‹)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  worker-improved:
    <<: *common-build
    container_name: automation-worker-improved
    command: python worker.py
    environment:
      <<: *common-env
      WORKER_TYPE: improved
      MAX_WORKERS: "3"
      WORKER_POLL_INTERVAL: "3"
      BROADCAST_MODE: unified
    profiles:
      - legacy
    networks:
      - automation-network

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ“Š Optional: PostgreSQL for local development
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  postgres:
    image: postgres:15-alpine
    container_name: automation-postgres
    
    environment:
      - POSTGRES_DB=automation_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    
    volumes:
      - postgres-data:/var/lib/postgresql/data
    
    ports:
      - "5432:5432"
    
    networks:
      - automation-network
    
    # Only start if explicitly requested
    profiles:
      - with-db

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ğŸ“ˆ Optional: Redis for job queue (future feature)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  redis:
    image: redis:7-alpine
    container_name: automation-redis
    
    volumes:
      - redis-data:/data
    
    ports:
      - "6379:6379"
    
    networks:
      - automation-network
    
    # Only start if explicitly requested
    profiles:
      - with-redis

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Volumes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
volumes:
  worker-logs:
    driver: local
  postgres-data:
    driver: local
  redis-data:
    driver: local

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Networks
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
networks:
  automation-network:
    driver: bridge